{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your data (replace with your actual data loading)\n",
        "# Assuming your data is in a CSV file named 'data.csv'\n",
        "data = pd.read_csv('azureml://subscriptions/512a781e-d15a-4734-adca-96ec827531cb/resourcegroups/xwang3306-rg/workspaces/DVA_PROJECT/datastores/workspaceblobstore/paths/UI/2024-10-16_040004_UTC/consolidated_cleaned_neiss_2014_2023.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1730585260912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(data.columns)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "['CPSC_Case_Number',\n 'Treatment_Date',\n 'Age',\n 'Sex',\n 'Race',\n 'Other_Race',\n 'Hispanic',\n 'Body_Part',\n 'Diagnosis',\n 'Other_Diagnosis',\n 'Body_Part_2',\n 'Diagnosis_2',\n 'Other_Diagnosis_2',\n 'Disposition',\n 'Location',\n 'Fire_Involvement',\n 'Product_1',\n 'Product_2',\n 'Product_3',\n 'Alcohol',\n 'Drug',\n 'Narrative',\n 'Stratum',\n 'PSU',\n 'Weight',\n 'Year',\n 'Month',\n 'Day']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585261221
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data['Disposition_recode']=np.nan\n",
        "data.loc[((data['Disposition']==6) | (data['Disposition']==1)), 'Disposition_recode'] = 0\n",
        "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
        "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
        "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
        "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
        "data=data[data['Disposition_recode'].notna()]\n",
        "\n",
        "data['Disposition_recode_2']=0\n",
        "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585262012
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Disposition_recode'].value_counts()\n",
        "data['Disposition_recode_2'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "0    3142030\n1     378362\nName: Disposition_recode_2, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585262216
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_after_dx(narrative):\n",
        "  if isinstance(narrative, str):\n",
        "    parts = narrative.split(\"DX\", 1)\n",
        "    if len(parts) > 1:\n",
        "      return parts[0]\n",
        "    else:\n",
        "      return narrative  # No \"DX:\" found, return the original string\n",
        "  else:\n",
        "    return narrative  # Not a string, return as is\n",
        "\n",
        "\n",
        "replace_list=['ANKLE', 'ARM', 'BODY_PART', 'CHEST', 'CONTUSION', 'CUT', 'EAR', 'ELBOW', 'EYE', 'FACE', 'FINGER', 'FOOT', 'FOREHEAD', 'FRACTURE', 'FX', 'HAND', 'HEAD', 'HIP', 'KNEE', 'LAC', 'LACERATION', 'LEG', 'LOC', 'LOSE', 'NECK', 'PAIN', 'SHOULDER', \n",
        "'SPRAIN', 'STRAIN', 'SWELL', 'THUMB', 'TOE', 'WRIST','ABRASION', 'ACHE', 'BREAK', 'BURN', 'CHIN', 'CUT', 'ER', 'FALL', 'FRACTURE', 'FX', 'HIT', 'INJURY', 'LACERATION', 'LIP', 'LOSE', 'LOC', 'MOUTH', 'NOSE', 'PAIN', 'RIB', 'SCALP', 'SPRAIN', 'STRAIN', 'SWELL', 'TOE', 'TWIST', 'WRIST']\n",
        "\n",
        "for i in replace_list:\n",
        "  data['Narrative'] = data['Narrative'].str.replace(i, '')\n",
        "\n",
        "data['Narrative'] = data['Narrative'].apply(remove_after_dx)\n",
        "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
        "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('JURED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('URED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' LT ', '')\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585566405
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_col=['Disposition_recode_2','Age', 'Sex', 'Race','Location','Hispanic', 'Body_Part','Product_1' ,'Alcohol', 'Drug','Narrative']\n",
        "data_core=data[core_col]\n",
        "data_core_sample=data_core.sample(frac=0.1).reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585567208
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "corpus = data_core_sample['Narrative'].fillna('')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "data_core_sample['Processed_Narrative'] = corpus.apply(preprocess_text)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer(max_features=140, stop_words='english')\n",
        "\n",
        "# Fit and transform the processed text\n",
        "tfidf_matrix = vectorizer.fit_transform(data_core_sample['Processed_Narrative'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenate the TF-IDF features with your existing data\n",
        "data_ready = pd.concat([data_core_sample, tfidf_df], axis=1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585984581
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(data_ready.columns)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "['Disposition_recode_2',\n 'Age',\n 'Sex',\n 'Race',\n 'Location',\n 'Hispanic',\n 'Body_Part',\n 'Product_1',\n 'Alcohol',\n 'Drug',\n 'Narrative',\n 'Processed_Narrative',\n 'aft',\n 'anoth',\n 'ation',\n 'backwards',\n 'balance',\n 'ball',\n 'bar',\n 'baseball',\n 'basketball',\n 'bathroom',\n 'bed',\n 'bend',\n 'bicycle',\n 'bike',\n 'body',\n 'bottle',\n 'box',\n 'break',\n 'car',\n 'carpet',\n 'carry',\n 'catch',\n 'cause',\n 'cd',\n 'chair',\n 'class',\n 'clean',\n 'climb',\n 'come',\n 'concrete',\n 'corn',\n 'couch',\n 'day',\n 'days',\n 'develop',\n 'dog',\n 'door',\n 'dress',\n 'drink',\n 'drop',\n 'excise',\n 'fall',\n 'fe',\n 'feet',\n 'felt',\n 'fence',\n 'flight',\n 'floor',\n 'fore',\n 'forward',\n 'game',\n 'glass',\n 'grind',\n 'gym',\n 'hd',\n 'heavy',\n 'helmet',\n 'home',\n 'hot',\n 'house',\n 'ing',\n 'jump',\n 'kick',\n 'kitchen',\n 'knife',\n 'ladd',\n 'land',\n 'leave',\n 'lift',\n 'lose',\n 'low',\n 'metal',\n 'miss',\n 'mom',\n 'moth',\n 'nail',\n 'night',\n 'note',\n 'nurse',\n 'open',\n 'ov',\n 'park',\n 'pass',\n 'piece',\n 'play',\n 'pool',\n 'pop',\n 'practice',\n 'present',\n 'pull',\n 'punch',\n 'push',\n 'rid',\n 'right',\n 'ring',\n 'roll',\n 'run',\n 'saw',\n 'school',\n 'scoot',\n 'shoe',\n 'sit',\n 'skateboard',\n 'slide',\n 'socc',\n 'sp',\n 'speed',\n 'stairs',\n 'stand',\n 'start',\n 'state',\n 'step',\n 'stick',\n 'strike',\n 'sustain',\n 'swallow',\n 'swim',\n 'swing',\n 'table',\n 'throw',\n 'tile',\n 'ting',\n 'toilet',\n 'toy',\n 'trampoline',\n 'trip',\n 'try',\n 'upp',\n 'use',\n 'walk',\n 'wall',\n 'wat',\n 'weight',\n 'wet',\n 'window',\n 'wing',\n 'wood',\n 'wooden',\n 'work',\n 'yestday']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585984889
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score\n",
        "from scipy.stats import mode\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data_ready.drop(['Disposition_recode_2','Narrative','Processed_Narrative'], axis=1)  # ,'Processed_Narrative'] Replace 'target_variable' with your target column name\n",
        "y = data_ready['Disposition_recode_2']\n",
        "\n",
        "# Encode target variable if it's categorical\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a scaler for numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
        "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
        "\n",
        "# Convert scaled features back to DataFrame\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n",
        "\n",
        "# Combine scaled numerical features with categorical features\n",
        "X_train_final = X_resampled.copy()\n",
        "X_test_final = X_test_scaled_df.copy()\n",
        "y_train_final=y_resampled.copy()\n",
        "\n",
        "\n",
        "# Define models and their parameter grids for hyperparameter tuning\n",
        "models = {\n",
        "    #'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
        "    #'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\n",
        "    #'XGBoost': (XGBClassifier(objective='binary:logistic', use_label_encoder=False), {'xgboost__learning_rate': [0.1, 0.01], 'xgboost__max_depth': [3, 5]}),\n",
        "    'Logistic_Regression': (LogisticRegression(solver='liblinear'), {'logistic_regression__C': [0.1, 1, 10]}),\n",
        "    'SVM': (SVC(probability=True), {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}),\n",
        "    'NN': (MLPClassifier(max_iter=1000), {'nn__hidden_layer_sizes': [(10,), (50,), (100,)], 'nn__activation': ['relu', 'tanh']}),\n",
        "}\n",
        "\n",
        "# Initialize list to store predictions from each model\n",
        "ensemble_predictions = []\n",
        "ensemble_probabilities = []\n",
        "\n",
        "for model_name, (model, param_grid) in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    # Create a pipeline for preprocessing and model training\n",
        "    pipeline = Pipeline([\n",
        "        (model_name.lower(), model)\n",
        "    ])\n",
        "\n",
        "    # Perform hyperparameter tuning using GridSearchCV with AUC score\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_weighted')\n",
        "    grid_search.fit(X_train_final, y_train_final)\n",
        "\n",
        "    # Get the best model from the grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions and predict probabilities on the test set\n",
        "    y_pred = best_model.predict(X_test_final)\n",
        "    y_prob = best_model.predict_proba(X_test_final)[:, 1]  # Probability for the positive class\n",
        "    ensemble_predictions.append(y_pred)\n",
        "    ensemble_probabilities.append(y_prob)\n",
        "\n",
        "    # Evaluate the model using accuracy, F1-score, and AUC\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "    print(f\"{model_name} F1 Score: {f1}\")\n",
        "    print(f\"{model_name} AUC: {auc}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Implement ensemble method (majority voting for predictions, average for probabilities)\n",
        "ensemble_predictions = np.array(ensemble_predictions)\n",
        "ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n",
        "\n",
        "# Average the probabilities for the ensemble AUC\n",
        "ensemble_probabilities = np.mean(ensemble_probabilities, axis=0)\n",
        "\n",
        "# Evaluate the ensemble model using accuracy, F1-score, and AUC\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_pred_final)\n",
        "ensemble_f1 = f1_score(y_test, ensemble_pred_final)\n",
        "ensemble_auc = roc_auc_score(y_test, ensemble_probabilities)\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
        "print(\"Ensemble F1 Score:\", ensemble_f1)\n",
        "print(\"Ensemble AUC:\", ensemble_auc)\n",
        "print(classification_report(y_test, ensemble_pred_final))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training Logistic_Regression...\nLogistic_Regression Accuracy: 0.6872230428360414\nLogistic_Regression F1 Score: 0.3224833866601034\nLogistic_Regression AUC: 0.7499783753981628\n              precision    recall  f1-score   support\n\n           0       0.95      0.69      0.80     62885\n           1       0.21      0.70      0.32      7523\n\n    accuracy                           0.69     70408\n   macro avg       0.58      0.69      0.56     70408\nweighted avg       0.87      0.69      0.75     70408\n\nTraining SVM...\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730591420669
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imblearn"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting imblearn\n  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\nCollecting imbalanced-learn\n  Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.23.5)\nRequirement already satisfied: scikit-learn>=1.0.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.5.1)\nRequirement already satisfied: joblib>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\nRequirement already satisfied: scipy>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.10.1)\nInstalling collected packages: imbalanced-learn, imblearn\nSuccessfully installed imbalanced-learn-0.12.4 imblearn-0.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730587511841
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xxx"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730585985228
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}