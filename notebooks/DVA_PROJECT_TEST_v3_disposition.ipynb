{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "#from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your data (replace with your actual data loading)\n",
        "# Assuming your data is in a CSV file named 'data.csv'\n",
        "data = pd.read_csv('azureml://subscriptions/512a781e-d15a-4734-adca-96ec827531cb/resourcegroups/xwang3306-rg/workspaces/DVA_PROJECT/datastores/workspaceblobstore/paths/UI/2024-10-16_040004_UTC/consolidated_cleaned_neiss_2014_2023.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.9.1)\r\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (2024.7.24)\r\nRequirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (8.1.7)\r\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (4.66.4)\r\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (1.2.0)\r\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1730688037993
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688038178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data['#_prod']=np.nan\n",
        "data.loc[((data['Product_1']>0)), '#_prod'] = 1\n",
        "data.loc[((data['Product_1']>0) & (data['Product_2']>0)), '#_prod'] = 2\n",
        "data.loc[((data['Product_1']>0) & (data['Product_2']>0) & (data['Product_3']>0)), '#_prod'] = 3"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688038341
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data['Disposition_recode']=np.nan\n",
        "data.loc[((data['Disposition']==1)), 'Disposition_recode'] = 0\n",
        "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
        "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
        "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
        "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
        "data=data[data['Disposition_recode'].notna()]\n",
        "\n",
        "data['Disposition_recode_2']=0\n",
        "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688039132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Disposition_recode'].value_counts()\n",
        "data['Disposition_recode_2'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "0    3085714\n1     378362\nName: Disposition_recode_2, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688039315
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_after_dx(narrative):\n",
        "  if isinstance(narrative, str):\n",
        "    parts = narrative.split(\"DX\", 1)\n",
        "    if len(parts) > 1:\n",
        "      return parts[0]\n",
        "    else:\n",
        "      return narrative  # No \"DX:\" found, return the original string\n",
        "  else:\n",
        "    return narrative  # Not a string, return as is\n",
        "\n",
        "\n",
        "replace_list=['ANKLE', 'ARM', 'BODY_PART', 'CHEST', 'CONTUSION', 'CUT', 'EAR', 'ELBOW', 'EYE', 'FACE', 'FINGER', 'FOOT', 'FOREHEAD', 'FRACTURE', 'FX', 'HAND', 'HEAD', 'HIP', 'KNEE', 'LAC', 'LACERATION', 'LEG', 'LOC', 'LOSE', 'NECK', 'PAIN', 'SHOULDER', \n",
        "'SPRAIN', 'STRAIN', 'SWELL', 'THUMB', 'TOE', 'WRIST','ABRASION', 'ACHE', 'BREAK', 'BURN', 'CHIN', 'CUT', 'ER', 'FALL', 'FRACTURE', 'FX', 'HIT', 'INJURY', 'LACERATION', 'LIP', 'LOSE', 'LOC', 'MOUTH', 'NOSE', 'PAIN', 'RIB', 'SCALP', 'SPRAIN', 'STRAIN', 'SWELL', 'TOE', 'TWIST', 'WRIST']\n",
        "\n",
        "for i in replace_list:\n",
        "  data['Narrative'] = data['Narrative'].str.replace(i, '')\n",
        "\n",
        "data['Narrative'] = data['Narrative'].apply(remove_after_dx)\n",
        "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
        "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('JURED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('URED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' LT ', '')\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688343495
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_col=['Disposition_recode_2','Age', 'Sex', 'Race','Location','Hispanic', 'Body_Part','Product_1' ,'Alcohol', 'Drug','Narrative']\n",
        "data_core=data[core_col]\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688344050
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bad=data_core[data_core['Disposition_recode_2']==1]\n",
        "df_good=data_core[data_core['Disposition_recode_2']==0]"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730688344421
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_good_sample=df_good.sample(frac=0.1).reset_index(drop=True)\n",
        "data_bad_sample=df_bad.sample(frac=0.8).reset_index(drop=True)\n",
        "data_core_sample=pd.concat([data_good_sample,data_bad_sample]).reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730689198205
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_core_sample=data_core_sample.sample(frac=1).reset_index(drop=True)\n",
        "data_core_sample"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "        Disposition_recode_2  Age  Sex  Race  #_prod  Location  Hispanic  \\\n0                          1   35    2     1     1.0         0       2.0   \n1                          0   13    2     3     1.0         1       0.0   \n2                          1   76    2     1     1.0         1       2.0   \n3                          1   66    1     1     2.0         1       0.0   \n4                          1   66    2     0     1.0         1       0.0   \n...                      ...  ...  ...   ...     ...       ...       ...   \n611256                     0   10    1     0     1.0         1       0.0   \n611257                     1   92    2     1     1.0         1       0.0   \n611258                     0   27    1     2     1.0         9       2.0   \n611259                     0   61    2     0     1.0         5       0.0   \n611260                     0    9    1     4     1.0         0       2.0   \n\n        Body_Part  Product_1  Alcohol  Drug  \\\n0              36       1842      0.0   0.0   \n1              93        679      0.0   0.0   \n2              79       1842      0.0   0.0   \n3              31       5040      0.0   0.0   \n4              79        649      0.0   0.0   \n...           ...        ...      ...   ...   \n611256         94       1807      0.0   0.0   \n611257         79       1807      0.0   0.0   \n611258         37       1205      0.0   0.0   \n611259         30        611      0.0   0.0   \n611260         82       3246      0.0   0.0   \n\n                                                Narrative  \n0       35  SPED DOWN A STAIR ON THE WAY TO WALK H DOG...  \n1       13PT WAS JUMPING ON THE COUCH WHEN SHE STUBBED...  \n2       76 FALL DOWN A FLIGHT OF STAIRS GOING INTO THE...  \n3       66  WENT FOR A BICYCLE RIDE 1130  WHEN HE GOT ...  \n4              66 FALL OFF THE TOILET AND LANDED ONTO  D   \n...                                                   ...  \n611256             10 SPED AND FALL ON WET KITCHEN FLOOR   \n611257             92  WAS FOUND ON FLOOR AT HOME AFT A    \n611258  27  S WAS PLAYING BASKETBALL LAST NIGHT JUMPED...  \n611259  61 FALL IN THE SHOW AT THE DETOX CENT AND SUST...  \n611260                             9 FALL OFF SWING ONTO   \n\n[611261 rows x 12 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disposition_recode_2</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Race</th>\n      <th>#_prod</th>\n      <th>Location</th>\n      <th>Hispanic</th>\n      <th>Body_Part</th>\n      <th>Product_1</th>\n      <th>Alcohol</th>\n      <th>Drug</th>\n      <th>Narrative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>35</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>36</td>\n      <td>1842</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35  SPED DOWN A STAIR ON THE WAY TO WALK H DOG...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>13</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>93</td>\n      <td>679</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13PT WAS JUMPING ON THE COUCH WHEN SHE STUBBED...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>76</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>79</td>\n      <td>1842</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>76 FALL DOWN A FLIGHT OF STAIRS GOING INTO THE...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>66</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>31</td>\n      <td>5040</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>66  WENT FOR A BICYCLE RIDE 1130  WHEN HE GOT ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>66</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>79</td>\n      <td>649</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>66 FALL OFF THE TOILET AND LANDED ONTO  D</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>611256</th>\n      <td>0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>94</td>\n      <td>1807</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10 SPED AND FALL ON WET KITCHEN FLOOR</td>\n    </tr>\n    <tr>\n      <th>611257</th>\n      <td>1</td>\n      <td>92</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>79</td>\n      <td>1807</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>92  WAS FOUND ON FLOOR AT HOME AFT A</td>\n    </tr>\n    <tr>\n      <th>611258</th>\n      <td>0</td>\n      <td>27</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>2.0</td>\n      <td>37</td>\n      <td>1205</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>27  S WAS PLAYING BASKETBALL LAST NIGHT JUMPED...</td>\n    </tr>\n    <tr>\n      <th>611259</th>\n      <td>0</td>\n      <td>61</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>30</td>\n      <td>611</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>61 FALL IN THE SHOW AT THE DETOX CENT AND SUST...</td>\n    </tr>\n    <tr>\n      <th>611260</th>\n      <td>0</td>\n      <td>9</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>82</td>\n      <td>3246</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9 FALL OFF SWING ONTO</td>\n    </tr>\n  </tbody>\n</table>\n<p>611261 rows × 12 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730689237976
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "corpus = data_core_sample['Narrative'].fillna('')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "data_core_sample['Processed_Narrative'] = corpus.apply(preprocess_text)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer(max_features=140, stop_words='english')\n",
        "\n",
        "# Fit and transform the processed text\n",
        "tfidf_matrix = vectorizer.fit_transform(data_core_sample['Processed_Narrative'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenate the TF-IDF features with your existing data\n",
        "data_ready = pd.concat([data_core_sample, tfidf_df], axis=1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730690009702
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_ready = pd.concat([data_core_sample, tfidf_df], axis=1)\n",
        "#list(data_ready.columns)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730689110262
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score\n",
        "from scipy.stats import mode\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data_ready.drop(['Disposition_recode_2','Narrative','Processed_Narrative'], axis=1)  # ,'Processed_Narrative'] Replace 'target_variable' with your target column name\n",
        "y = data_ready['Disposition_recode_2']\n",
        "\n",
        "# Encode target variable if it's categorical\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a scaler for numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
        "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
        "\n",
        "# Convert scaled features back to DataFrame\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n",
        "\n",
        "# Combine scaled numerical features with categorical features\n",
        "X_train_final = X_resampled.copy()\n",
        "X_test_final = X_test_scaled_df.copy()\n",
        "y_train_final=y_resampled.copy()\n",
        "\n",
        "\n",
        "# Define models and their parameter grids for hyperparameter tuning\n",
        "models = {\n",
        "    #'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
        "    #'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\n",
        "    'XGBoost': (XGBClassifier(objective='binary:logistic', use_label_encoder=False), \n",
        "           {'xgboost__learning_rate': [0.1, 0.01, 0.001],            # Step size shrinkage to prevent overfitting\n",
        "            'xgboost__max_depth': [3, 5, 7],                         # Maximum depth of each tree\n",
        "            #'xgboost__n_estimators': [100, 200, 300],                # Number of boosting rounds\n",
        "            #'xgboost__min_child_weight': [1, 3, 5],                  # Minimum sum of instance weight needed in a child\n",
        "            #'xgboost__subsample': [0.6, 0.8,0.9],                   # Subsample ratio of the training instances\n",
        "            #'xgboost__colsample_bytree': [0.6, 0.8, 0.9],            # Subsample ratio of columns when constructing each tree\n",
        "            #'xgboost__gamma': [0.1, 0.5, 1,3],                     # Minimum loss reduction required for a split\n",
        "            #'xgboost__reg_alpha': [0.01, 0.1,0.5,1],                    # L1 regularization term on weights\n",
        "            #'xgboost__reg_lambda': [1, 1.5, 2],                      # L2 regularization term on weights\n",
        "            #'xgboost__scale_pos_weight': [1, 5, 10]\n",
        "            }),\n",
        "    #'Logistic_Regression': (LogisticRegression(solver='liblinear'), {'logistic_regression__C': [0.1, 1, 10]}),\n",
        "    #'SVM': (SVC(probability=True), {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}),\n",
        "    #'NN': (MLPClassifier(max_iter=1000), {'nn__hidden_layer_sizes': [(10,), (50,), (100,)], 'nn__activation': ['relu', 'tanh']}),\n",
        "}\n",
        "\n",
        "# Initialize list to store predictions from each model\n",
        "ensemble_predictions = []\n",
        "ensemble_probabilities = []\n",
        "\n",
        "for model_name, (model, param_grid) in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    # Create a pipeline for preprocessing and model training\n",
        "    pipeline = Pipeline([\n",
        "        (model_name.lower(), model)\n",
        "    ])\n",
        "\n",
        "    # Perform hyperparameter tuning using GridSearchCV with AUC score\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_weighted',n_jobs=-1)\n",
        "    grid_search.fit(X_train_final, y_train_final)\n",
        "\n",
        "    # Get the best model from the grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions and predict probabilities on the test set\n",
        "    y_pred = best_model.predict(X_test_final)\n",
        "    y_prob = best_model.predict_proba(X_test_final)[:, 1]  # Probability for the positive class\n",
        "    ensemble_predictions.append(y_pred)\n",
        "    ensemble_probabilities.append(y_prob)\n",
        "\n",
        "    # Evaluate the model using accuracy, F1-score, and AUC\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "    print(f\"{model_name} F1 Score: {f1}\")\n",
        "    print(f\"{model_name} AUC: {auc}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Implement ensemble method (majority voting for predictions, average for probabilities)\n",
        "ensemble_predictions = np.array(ensemble_predictions)\n",
        "ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n",
        "\n",
        "# Average the probabilities for the ensemble AUC\n",
        "ensemble_probabilities = np.mean(ensemble_probabilities, axis=0)\n",
        "\n",
        "# Evaluate the ensemble model using accuracy, F1-score, and AUC\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_pred_final)\n",
        "ensemble_f1 = f1_score(y_test, ensemble_pred_final)\n",
        "ensemble_auc = roc_auc_score(y_test, ensemble_probabilities)\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
        "print(\"Ensemble F1 Score:\", ensemble_f1)\n",
        "print(\"Ensemble AUC:\", ensemble_auc)\n",
        "print(classification_report(y_test, ensemble_pred_final))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training XGBoost...\n[03:40:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:40:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:45:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:40:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:45:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:40:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:45:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:55:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nXGBoost Accuracy: 0.7756864862211971\nXGBoost F1 Score: 0.7741903609100566\nXGBoost AUC: 0.8551943746341788\n              precision    recall  f1-score   support\n\n           0       0.78      0.78      0.78     61590\n           1       0.77      0.77      0.77     60663\n\n    accuracy                           0.78    122253\n   macro avg       0.78      0.78      0.78    122253\nweighted avg       0.78      0.78      0.78    122253\n\nEnsemble Accuracy: 0.7756864862211971\nEnsemble F1 Score: 0.7741903609100566\nEnsemble AUC: 0.8551943746341788\n              precision    recall  f1-score   support\n\n           0       0.78      0.78      0.78     61590\n           1       0.77      0.77      0.77     60663\n\n    accuracy                           0.78    122253\n   macro avg       0.78      0.78      0.78    122253\nweighted avg       0.78      0.78      0.78    122253\n\n[03:40:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[03:45:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_18638/3370004319.py:94: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730692588328
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xxx"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730689110316
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imblearn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730689110332
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xxx"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730689110347
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}