{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1731120966160
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: xgboost in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from xgboost) (2.1.2)\n",
      "Requirement already satisfied: scipy in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from xgboost) (1.14.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pickle\n",
    "!pip install xgboost\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your data (replace with your actual data loading)\n",
    "# Assuming your data is in a CSV file named 'data.csv'\n",
    "import pandas as pd\n",
    "new_columns=pd.read_csv('../data/neiss_10p_sample_new_columns.csv')\n",
    "data_10P = pd.read_csv('../data/NEISS_10P_Sample.csv')\n",
    "# sematic=pd.read_csv('../data/10P_sematic_bert.csv')\n",
    "embedding=pd.read_csv('../data/gist_embedding_10p.csv')\n",
    "#embedding_2=pd.read_csv('../xwang3306/aliba_embedding_10p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731039776971
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#data_10p=data.sample(frac=0.1,random_state=42).reset_index(drop=True)\n",
    "#data_10p.to_csv('NEISS_10P_Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120993237
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CPSC_Case_Number</th>\n",
       "      <th>Treatment_Date</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Other_Race</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Body_Part</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221032332</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034040</td>\n",
       "      <td>-0.036666</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>-0.048995</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>-0.020119</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>-0.022191</td>\n",
       "      <td>0.038710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>181109464</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.015545</td>\n",
       "      <td>-0.040856</td>\n",
       "      <td>-0.042023</td>\n",
       "      <td>-0.050380</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>-0.002266</td>\n",
       "      <td>0.043031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210103105</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060739</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.022998</td>\n",
       "      <td>-0.055498</td>\n",
       "      <td>-0.040177</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>-0.086975</td>\n",
       "      <td>-0.045614</td>\n",
       "      <td>-0.022894</td>\n",
       "      <td>-0.013169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>161157997</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027231</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>-0.013848</td>\n",
       "      <td>-0.046165</td>\n",
       "      <td>-0.034185</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>-0.041465</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>0.013185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>181107411</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053574</td>\n",
       "      <td>-0.069382</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>0.037567</td>\n",
       "      <td>-0.024320</td>\n",
       "      <td>0.038775</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>0.001262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  CPSC_Case_Number Treatment_Date  Age  Sex  Race Other_Race  \\\n",
       "0           0         221032332     2022-09-24   14    1     0          0   \n",
       "1           1         181109464     2018-10-30   28    1     1          0   \n",
       "2           2         210103105     2020-10-24   35    1     0          0   \n",
       "3           3         161157997     2016-11-15  214    2     0          0   \n",
       "4           4         181107411     2018-10-21    4    1     0          0   \n",
       "\n",
       "   Hispanic  Body_Part  Diagnosis  ...       374       375       376  \\\n",
       "0       0.0         34         71  ...  0.034040 -0.036666  0.008797   \n",
       "1       0.0         79         71  ...  0.024239 -0.000142  0.007268   \n",
       "2       0.0         30         53  ...  0.060739  0.014632  0.022998   \n",
       "3       0.0         76         53  ...  0.027231 -0.016303 -0.013848   \n",
       "4       0.0         92         72  ...  0.053574 -0.069382  0.023767   \n",
       "\n",
       "        377       378       379       380       381       382       383  \n",
       "0  0.024984 -0.048995  0.002709 -0.020119 -0.058474 -0.022191  0.038710  \n",
       "1 -0.015545 -0.040856 -0.042023 -0.050380  0.008760 -0.002266  0.043031  \n",
       "2 -0.055498 -0.040177  0.024382 -0.086975 -0.045614 -0.022894 -0.013169  \n",
       "3 -0.046165 -0.034185  0.006101 -0.041465 -0.041030  0.040551  0.013185  \n",
       "4  0.044406  0.034625  0.037567 -0.024320  0.038775 -0.017813  0.001262  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data=data_10P.merge(new_columns,how='inner',on='CPSC_Case_Number').merge(sematic,how='inner',on='CPSC_Case_Number').merge(embedding,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
    "data=data_10P.merge(new_columns,how='inner',on='CPSC_Case_Number').merge(embedding,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
    "# data.rename(columns={\"sematic_distance_bert\": \"sematic_distance\"},inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'CPSC_Case_Number', 'Treatment_Date', 'Age', 'Sex',\n",
       "       'Race', 'Other_Race', 'Hispanic', 'Body_Part', 'Diagnosis',\n",
       "       ...\n",
       "       '374', '375', '376', '377', '378', '379', '380', '381', '382', '383'],\n",
       "      dtype='object', length=416)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121001405
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['#_prod']=np.nan\n",
    "data.loc[((data['Product_1']>0)), '#_prod'] = 1\n",
    "data.loc[((data['Product_1']>0) & (data['Product_2']>0)), '#_prod'] = 2\n",
    "data.loc[((data['Product_1']>0) & (data['Product_2']>0) & (data['Product_3']>0)), '#_prod'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121003728
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['Disposition_recode']=np.nan\n",
    "data.loc[((data['Disposition']==1)), 'Disposition_recode'] = 0\n",
    "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
    "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
    "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
    "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
    "data=data[data['Disposition_recode'].notna()]\n",
    "\n",
    "data['Disposition_recode_2']=0\n",
    "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disposition_recode_2\n",
       "0    296227\n",
       "1     32874\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[(data['Body_Part']!=0) & (data['Body_Part']!=84) & (data['Body_Part']!=85) & (data['Body_Part']!=86) & (data['Body_Part']!=87)]\n",
    "data['Disposition_recode'].value_counts()\n",
    "data['Disposition_recode_2'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121014315
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "bdpt_dict={}\n",
    "bdpt_dict[0]='INTERNAL'\n",
    "bdpt_dict[30]='SHOULDER'\n",
    "bdpt_dict[31]='UPPERTRUNK'\n",
    "bdpt_dict[32]='ELBOW'\n",
    "bdpt_dict[33]='LOWERARM'\n",
    "bdpt_dict[34]='WRIST'\n",
    "bdpt_dict[35]='KNEE'\n",
    "bdpt_dict[36]='LOWERLEG'\n",
    "bdpt_dict[37]='ANKLE'\n",
    "bdpt_dict[38]='PUBICREGION'\n",
    "bdpt_dict[75]='HEAD'\n",
    "bdpt_dict[76]='FACE'\n",
    "bdpt_dict[77]='EYEBALL'\n",
    "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
    "bdpt_dict[79]='LOWERTRUNK'\n",
    "bdpt_dict[80]='UPPERARM'\n",
    "bdpt_dict[81]='UPPERLEG'\n",
    "bdpt_dict[82]='HAND'\n",
    "bdpt_dict[83]='FOOT'\n",
    "bdpt_dict[84]='25-50% OF BODY'\n",
    "bdpt_dict[85]='ALLPARTSBODY'\n",
    "bdpt_dict[86]='OTHER(OLD)'\n",
    "bdpt_dict[87]='NOTSTATED/UNK'\n",
    "bdpt_dict[88]='MOUTH'\n",
    "bdpt_dict[89]='NECK'\n",
    "bdpt_dict[90]='LOWERARM(OLD)'\n",
    "bdpt_dict[91]='LOWERLEG(OLD)'\n",
    "bdpt_dict[92]='FINGER'\n",
    "bdpt_dict[93]='TOE'\n",
    "bdpt_dict[94]='EAR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121020538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data['body_string']=data['Body_Part'].map(bdpt_dict)\n",
    "data['Narrative_LLM']=data[\"activity_at_injury\"].astype(str) + ' '+data[\"injury_mechanism\"].astype(str)+ ' ' + data[\"object_involved\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114157682
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_after_dx(narrative):\n",
    "  if isinstance(narrative, str):\n",
    "    parts = narrative.split(\"DX\", 1)\n",
    "    if len(parts) > 1:\n",
    "      return parts[0]\n",
    "    else:\n",
    "      return narrative  # No \"DX:\" found, return the original string\n",
    "  else:\n",
    "    return narrative  # Not a string, return as is\n",
    "\n",
    "data['Narrative'] = data['Narrative'].apply(remove_after_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114166839
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import re\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "medical_terms = {\n",
    "    \"&\": \"and\",\n",
    "    \"***\": \"\",\n",
    "    \">>\": \"clinical diagnosis\",\n",
    "    \"@\": \"at\",\n",
    "    \"abd\": \"abdomen\",\n",
    "    \"af\": \"accidental fall\",\n",
    "    \"afib\": \"atrial fibrillation\",\n",
    "    \"aki\": \"acute kidney injury\",\n",
    "    \"am\": \"morning\",\n",
    "    \"ams\": \"altered mental status\",\n",
    "    \"bac\": \"blood alcohol content\",\n",
    "    \"bal\": \"blood alcohol level,\",\n",
    "    \"biba\": \"brought in by ambulance\",\n",
    "    \"c/o\": \"complains of\",\n",
    "    \"chi\": \"closed-head injury\",\n",
    "    \"clsd\": \"closed\",\n",
    "    \"cpk\": \"creatine phosphokinase\",\n",
    "    \"cva\": \"cerebral vascular accident\",\n",
    "    \"dx\": \"clinical diagnosis\",\n",
    "    \"ecf\": \"extended-care facility\",\n",
    "    \"er\": \"emergency room\",\n",
    "    \"etoh\": \"ethyl alcohol\",\n",
    "    \"eval\": \"evaluation\",\n",
    "    \"fd\": \"fall detected\",\n",
    "    \"fx\": \"fracture\",\n",
    "    \"fxs\": \"fractures\",\n",
    "    \"glf\": \"ground level fall\",\n",
    "    \"h/o\": \"history of\",\n",
    "    \"htn\": \"hypertension\",\n",
    "    \"hx\": \"history of\",\n",
    "    \"inj\": \"injury\",\n",
    "    \"inr\": \"international normalized ratio\",\n",
    "    \"intox\": \"intoxication\",\n",
    "    \"l\": \"left\",\n",
    "    \"loc\": \"loss of consciousness\",\n",
    "    \"lt\": \"left\",\n",
    "    \"mech\": \"mechanical\",\n",
    "    \"mult\": \"multiple\",\n",
    "    \"n.h.\": \"nursing home\",\n",
    "    \"nh\": \"nursing home\",\n",
    "    \"p/w\": \"presents with\",\n",
    "    \"pm\": \"afternoon\",\n",
    "    \"pt\": \"patient\",\n",
    "    \"pta\": \"prior to arrival\",\n",
    "    \"pts\": \"patient's\",\n",
    "    \"px\": \"physical examination\", # not \"procedure\",\n",
    "    \"r\": \"right\",\n",
    "    \"r/o\": \"rules out\",\n",
    "    \"rt\": \"right\",\n",
    "    \"s'd&f\": \"slipped and fell\",\n",
    "    \"s/p\": \"after\",\n",
    "    \"sah\": \"subarachnoid hemorrhage\",\n",
    "    \"sdh\": \"acute subdural hematoma\",\n",
    "    \"sts\": \"sit-to-stand\",\n",
    "    \"t'd&f\": \"tripped and fell\",\n",
    "    \"tr\": \"trauma\",\n",
    "    \"uti\": \"urinary tract infection\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"wks\": \"weeks\"\n",
    "}\n",
    "\n",
    "# cleanning\n",
    "def clean_narrative(text):\n",
    "    # lowercase everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # unglue DX\n",
    "    regex_dx = r\"([ˆ\\W]*(dx)[ˆ\\W]*)\"\n",
    "    text = re.sub(regex_dx, r\". dx: \", text)\n",
    "\n",
    "    # remove age and sex identifications\n",
    "    ## regex to capture age and sex (not perfect but captures almost all of the cases)\n",
    "    regex_age_sex = r\"(\\d+)\\s*?(yof|yf|yo\\s*female|yo\\s*f|yom|ym|yr|yo\\s*male|yo\\s*m)\"\n",
    "    age_sex_match = re.search(regex_age_sex, text)\n",
    "\n",
    "    ## format age and sex\n",
    "    if age_sex_match:\n",
    "        age = age_sex_match.group(1)\n",
    "        sex = age_sex_match.group(2)\n",
    "        \n",
    "        # probably not best practice but it works with this data\n",
    "        if \"f\" in sex:\n",
    "            #text = text.replace(age_sex_match.group(0), f\"{age} years old female\")\n",
    "            text = text.replace(age_sex_match.group(0), f\"patient\")\n",
    "        elif \"m\" in sex:\n",
    "            #text = text.replace(age_sex_match.group(0), f\"{age} years old male\")\n",
    "            text = text.replace(age_sex_match.group(0), f\"patient\")\n",
    "    \n",
    "    # translate medical terms\n",
    "    for term, replacement in medical_terms.items():\n",
    "        if term == \"@\" or term == \">>\" or term == \"&\" or term == \"***\":\n",
    "            pattern = fr\"({re.escape(term)})\"\n",
    "            text = re.sub(pattern, f\" {replacement} \", text) # force spaces around replacement\n",
    "            \n",
    "        else:\n",
    "            pattern = fr\"(?<!-)\\b({re.escape(term)})\\b(?!-)\"\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    # user-friendly format\n",
    "    sentences = sent_tokenizer.tokenize(text)\n",
    "    sentences = [sent.capitalize() for sent in sentences]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "text = '72 YOF SLIPPED AND FELL ON THE FLOOR THIS AM. DX: L-3, L-4 FRACTURE, RIGHT RIB FRACTURES X 3.'\n",
    "print(\"Original text:\", text)\n",
    "print(\"Clean text:\", clean_narrative(text))\n",
    "print()\n",
    "\n",
    "# Use all CPU cores\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    data['Narrative'] = pool.map(clean_narrative, data['Narrative'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114169143
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PATIENT REPORTS HE FELL 1 WEEK AND COMPLAINS O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A PATIENT BENT TO PICK UP CRATE AT HOME TO ED ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATIENTRIDING ON MOUNTAIN BIKE PRACTICING FELL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242139</th>\n",
       "      <td>PATIENT FOOT PAIN SP FALL OFF BIKE TODAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242140</th>\n",
       "      <td>PATIENT LAC HEAD  ON HAMOMOCK HIT HEAD ON FENC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242141</th>\n",
       "      <td>PATIENT FROM THE NURSING HOME FELL OUT OF A WH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242142</th>\n",
       "      <td>PATIENT CO PAIN TO BOTTOM OF HER FEET 22 CALLU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242143</th>\n",
       "      <td>RIGHT HAND CONTUSION PATIENT PUNCHED A WALL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238207 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Narrative\n",
       "0       PATIENT REPORTS HE FELL 1 WEEK AND COMPLAINS O...\n",
       "1       A PATIENT BENT TO PICK UP CRATE AT HOME TO ED ...\n",
       "2       PATIENTRIDING ON MOUNTAIN BIKE PRACTICING FELL...\n",
       "3       14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...\n",
       "4       4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...\n",
       "...                                                   ...\n",
       "242139           PATIENT FOOT PAIN SP FALL OFF BIKE TODAY\n",
       "242140  PATIENT LAC HEAD  ON HAMOMOCK HIT HEAD ON FENC...\n",
       "242141  PATIENT FROM THE NURSING HOME FELL OUT OF A WH...\n",
       "242142  PATIENT CO PAIN TO BOTTOM OF HER FEET 22 CALLU...\n",
       "242143        RIGHT HAND CONTUSION PATIENT PUNCHED A WALL\n",
       "\n",
       "[238207 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Narrative']=data['Narrative'].str.upper()\n",
    "data[['Narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114199643
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "replace_list=['ANKLE', 'ARM', 'BODY_PART', 'CHEST', 'CONTUSION', 'CUT', 'EAR', 'ELBOW', 'EYE', 'FACE', 'FINGER', 'FOOT', 'FOREHEAD', 'FRACTURE', 'FX', 'HAND', 'HEAD', 'HIP', 'KNEE', 'LAC', 'LACERATION', 'LEG', 'LOC', 'LOSE', 'NECK', 'PAIN', 'SHOULDER', \n",
    "'SPRAIN', 'STRAIN', 'SWELL', 'THUMB', 'TOE', 'WRIST','ABRASION', 'ACHE', 'BREAK', 'BURN', 'CHIN', 'CUT', 'ER', 'FRACTURE', 'FX', 'HIT', 'INJURY', 'LACERATION', 'LIP', 'LOSE', 'LOC', 'MOUTH', 'NOSE', 'PAIN', 'RIB', 'SCALP', 'SPRAIN', 'STRAIN', 'SWELL', 'TOE', 'TWIST', 'WRIST']\n",
    "\n",
    "for i in replace_list:\n",
    "  data['Narrative'] = data['Narrative'].str.replace(i, '')\n",
    "  data['Narrative_LLM']= data['Narrative_LLM'].str.replace(i, '')\n",
    "\n",
    "\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' F ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' M ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
    "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('JURED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('URED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' LT ', '')\n",
    "\n",
    "data['Narrative_LLM'] = data['Narrative_LLM'].str.replace('fell', 'fall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114269508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_core=data.copy() #[core_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114245702
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191347"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "212347-21000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114277211
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# please change these hard coded numbers accordingly\n",
    "data_core=data_core.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "data_test=data_core.head(21000).reset_index(drop=True)\n",
    "data_fit=data_core.tail(191347).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114281538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CPSC_Case_Number</th>\n",
       "      <th>Treatment_Date</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Other_Race</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Body_Part</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>...</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>#_prod</th>\n",
       "      <th>Disposition_recode</th>\n",
       "      <th>Disposition_recode_2</th>\n",
       "      <th>body_string</th>\n",
       "      <th>Narrative_LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276195</td>\n",
       "      <td>180844861</td>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011324</td>\n",
       "      <td>-0.028799</td>\n",
       "      <td>-0.008153</td>\n",
       "      <td>0.016681</td>\n",
       "      <td>0.050130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UPPERTRUNK</td>\n",
       "      <td>walking up a flight of stairs unknown stairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14226</td>\n",
       "      <td>220610154</td>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022987</td>\n",
       "      <td>-0.038257</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>-0.118770</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UPPERTRUNK</td>\n",
       "      <td>picking something up off floor fall stairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94462</td>\n",
       "      <td>210242685</td>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002911</td>\n",
       "      <td>-0.079067</td>\n",
       "      <td>-0.018133</td>\n",
       "      <td>-0.018173</td>\n",
       "      <td>-0.007347</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UPPERTRUNK</td>\n",
       "      <td>horseback riding fall horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85327</td>\n",
       "      <td>150425009</td>\n",
       "      <td>2015-04-02</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>-0.040780</td>\n",
       "      <td>-0.020915</td>\n",
       "      <td>0.026675</td>\n",
       "      <td>0.042833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>FINGER</td>\n",
       "      <td>unknown unknown ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199859</td>\n",
       "      <td>181132035</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008223</td>\n",
       "      <td>-0.061483</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>-0.063617</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>UPPERTRUNK</td>\n",
       "      <td>unknown fall tub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 421 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  CPSC_Case_Number Treatment_Date  Age  Sex  Race Other_Race  \\\n",
       "0      276195         180844861     2018-08-20   59    1     0          0   \n",
       "1       14226         220610154     2022-05-27   61    1     1          0   \n",
       "2       94462         210242685     2021-02-19   18    1     0          0   \n",
       "3       85327         150425009     2015-04-02   54    2     0          0   \n",
       "4      199859         181132035     2018-11-05   35    2     1          0   \n",
       "\n",
       "   Hispanic  Body_Part  Diagnosis  ...       379       380       381  \\\n",
       "0       0.0         31         71  ... -0.011324 -0.028799 -0.008153   \n",
       "1       2.0         31         57  ... -0.022987 -0.038257  0.031624   \n",
       "2       0.0         31         53  ... -0.002911 -0.079067 -0.018133   \n",
       "3       0.0         92         56  ...  0.002016 -0.040780 -0.020915   \n",
       "4       0.0         31         57  ... -0.008223 -0.061483  0.001125   \n",
       "\n",
       "        382       383  #_prod  Disposition_recode  Disposition_recode_2  \\\n",
       "0  0.016681  0.050130     1.0                 2.0                     1   \n",
       "1 -0.118770  0.002407     2.0                 2.0                     1   \n",
       "2 -0.018173 -0.007347     1.0                 1.0                     1   \n",
       "3  0.026675  0.042833     1.0                 0.0                     0   \n",
       "4 -0.063617  0.015716     1.0                 0.0                     0   \n",
       "\n",
       "   body_string                                 Narrative_LLM  \n",
       "0   UPPERTRUNK  walking up a flight of stairs unknown stairs  \n",
       "1   UPPERTRUNK    picking something up off floor fall stairs  \n",
       "2   UPPERTRUNK                   horseback riding fall horse  \n",
       "3       FINGER                          unknown unknown ring  \n",
       "4   UPPERTRUNK                              unknown fall tub  \n",
       "\n",
       "[5 rows x 421 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bad=data_fit[data_fit['Disposition_recode_2']==1]\n",
    "df_good=data_fit[data_fit['Disposition_recode_2']==0]\n",
    "data_good_sample=df_good.sample(frac=0.2,random_state=42).reset_index(drop=True)\n",
    "data_bad_sample=df_bad.sample(frac=0.8,random_state=42).reset_index(drop=True)\n",
    "data_core_sample=pd.concat([data_good_sample,data_bad_sample]).reset_index(drop=True)\n",
    "data_core_sample=data_core_sample.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "data_core_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114374541
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "corpus = data_core_sample['Narrative'].fillna('')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_core_sample['Processed_Narrative'] = corpus.apply(preprocess_text)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(max_features=70, stop_words='english')\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix = vectorizer.fit_transform(data_core_sample['Processed_Narrative'])\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114410607
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus_LLM = data_core_sample['Narrative_LLM'].fillna('')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_core_sample['Processed_Narrative_LLM'] = corpus_LLM.apply(preprocess_text)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer_LLM = TfidfVectorizer(max_features=70, stop_words='english')\n",
    "\n",
    "\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix_LLM = vectorizer_LLM.fit_transform(data_core_sample['Processed_Narrative_LLM'])\n",
    "\n",
    "with open('tfidf_vectorizer_LLM.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer_LLM, file)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df_LLM = pd.DataFrame(tfidf_matrix_LLM.toarray(), columns=vectorizer_LLM.get_feature_names_out())\n",
    "tfidf_df_LLM=tfidf_df_LLM.add_suffix('_LLM')\n",
    "\n",
    "# Concatenate the TF-IDF features with your existing data\n",
    "data_ready = pd.concat([data_core_sample, tfidf_df,tfidf_df_LLM], axis=1)\n",
    "#data_ready = pd.concat([data_core_sample, tfidf_df_LLM], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114451560
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "new_data=data_test.copy()\n",
    "corpus_2 = new_data['Narrative'].fillna('')\n",
    "new_data['Processed_Narrative'] = corpus_2.apply(preprocess_text)\n",
    "tfidf_new = vectorizer.transform(new_data['Processed_Narrative'])\n",
    "tfidf_df_new = pd.DataFrame(tfidf_new.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "corpus_LLM_2 = new_data['Narrative_LLM'].fillna('')\n",
    "new_data['Processed_Narrative_LLM'] = corpus_LLM_2.apply(preprocess_text)\n",
    "tfidf_LLM_new = vectorizer_LLM.transform(new_data['Processed_Narrative_LLM'])\n",
    "tfidf_df_new_LLM = pd.DataFrame(tfidf_LLM_new.toarray(), columns=vectorizer_LLM.get_feature_names_out())\n",
    "tfidf_df_new_LLM=tfidf_df_new_LLM.add_suffix('_LLM')\n",
    "\n",
    "data_ready_test = pd.concat([data_test, tfidf_df_new,tfidf_df_new_LLM], axis=1)\n",
    "#data_ready_test = pd.concat([data_test, tfidf_df_new_LLM], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = data_core_sample\n",
    "data_ready_test = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114704628
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "drop_list=['Narrative','Unnamed: 0',\n",
    " 'CPSC_Case_Number',\n",
    " 'Treatment_Date','Race',\n",
    " 'Other_Race',\n",
    " 'Hispanic','Diagnosis',\n",
    " 'Other_Diagnosis',\n",
    " 'Body_Part_2',\n",
    " 'Diagnosis_2',\n",
    " 'Other_Diagnosis_2',\n",
    " 'Disposition','Fire_Involvement','Product_2',\n",
    " 'Product_3',\n",
    " 'Alcohol',\n",
    " 'Drug','Stratum',\n",
    " 'PSU',\n",
    " 'Weight',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'activity_at_injury',\n",
    " 'injury_mechanism',\n",
    " 'object_involved',\n",
    " '#_prod',\n",
    " 'Disposition_recode',\n",
    " 'Disposition_recode_2',\n",
    " 'body_string',\n",
    " 'Narrative_LLM',\n",
    " 'Processed_Narrative',\n",
    " # 'Processed_Narrative_LLM'\n",
    "          ]\n",
    "\n",
    "drop_list_test=['Narrative','Unnamed: 0',\n",
    " 'CPSC_Case_Number',\n",
    " 'Treatment_Date','Race',\n",
    " 'Other_Race',\n",
    " 'Hispanic','Diagnosis',\n",
    " 'Other_Diagnosis',\n",
    " 'Body_Part_2',\n",
    " 'Diagnosis_2',\n",
    " 'Other_Diagnosis_2',\n",
    " 'Disposition','Fire_Involvement','Product_2',\n",
    " 'Product_3',\n",
    " 'Alcohol',\n",
    " 'Drug','Stratum',\n",
    " 'PSU',\n",
    " 'Weight',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'activity_at_injury',\n",
    " 'injury_mechanism',\n",
    " 'object_involved',\n",
    " '#_prod',\n",
    " 'Disposition_recode',\n",
    " 'Disposition_recode_2',\n",
    " 'body_string',\n",
    " 'Narrative_LLM']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Sex', 'Body_Part', 'Location', 'Product_1', '0', '1', '2', '3',\n",
       "       '4',\n",
       "       ...\n",
       "       '374', '375', '376', '377', '378', '379', '380', '381', '382', '383'],\n",
       "      dtype='object', length=389)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_ready.drop(drop_list, axis=1) \n",
    "y = data_ready['Disposition_recode_2']\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731115388172
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score\n",
    "from scipy.stats import mode\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = data_ready.drop(drop_list, axis=1) \n",
    "y = data_ready['Disposition_recode_2']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test=data_ready_test.drop(drop_list_test, axis=1)\n",
    "y_test=data_ready_test['Disposition_recode_2']\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Create a scaler for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
    "\n",
    "with open('X_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
    "\n",
    "# Convert scaled features back to DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n",
    "\n",
    "# Combine scaled numerical features with categorical features\n",
    "X_train_final = X_resampled.copy()\n",
    "X_test_final = X_test_scaled_df.copy()\n",
    "y_train_final=y_resampled.copy()\n",
    "\n",
    "\n",
    "# Define models and their parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    #'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
    "    #'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\n",
    "    'XGBoost': (XGBClassifier(objective='binary:logistic'), \n",
    "           {'xgboost__learning_rate': [0.1, 0.01, 0.001],            # Step size shrinkage to prevent overfitting\n",
    "            'xgboost__max_depth': [3, 5, 7],                         # Maximum depth of each tree\n",
    "            #'xgboost__n_estimators': [100, 200, 300],                # Number of boosting rounds\n",
    "            #'xgboost__min_child_weight': [1, 3, 5],                  # Minimum sum of instance weight needed in a child\n",
    "            #'xgboost__subsample': [0.6, 0.8,0.9],                   # Subsample ratio of the training instances\n",
    "            #'xgboost__colsample_bytree': [0.6, 0.8, 0.9],            # Subsample ratio of columns when constructing each tree\n",
    "            #'xgboost__gamma': [0.1, 0.5, 1,3],                     # Minimum loss reduction required for a split\n",
    "            #'xgboost__reg_alpha': [0.01, 0.1,0.5,1],                    # L1 regularization term on weights\n",
    "            #'xgboost__reg_lambda': [1, 1.5, 2],                      # L2 regularization term on weights\n",
    "            'xgboost__scale_pos_weight': [1, 5, 10]\n",
    "            }),\n",
    "    #'Logistic_Regression': (LogisticRegression(solver='liblinear'), {'logistic_regression__C': [0.1, 1, 10]}),\n",
    "    #'SVM': (SVC(probability=True), {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}),\n",
    "    #'NN': (MLPClassifier(max_iter=1000), {'nn__hidden_layer_sizes': [(10,), (50,), (100,)], 'nn__activation': ['relu', 'tanh']}),\n",
    "}\n",
    "\n",
    "# Initialize list to store predictions from each model\n",
    "ensemble_predictions = []\n",
    "ensemble_probabilities = []\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # Create a pipeline for preprocessing and model training\n",
    "    pipeline = Pipeline([\n",
    "        (model_name.lower(), model)\n",
    "    ])\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV with AUC score\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='precision',n_jobs=-1)\n",
    "    grid_search.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions and predict probabilities on the test set\n",
    "    y_pred = best_model.predict(X_test_final)\n",
    "    y_prob = best_model.predict_proba(X_test_final)[:, 1]  # Probability for the positive class\n",
    "    ensemble_predictions.append(y_pred)\n",
    "    ensemble_probabilities.append(y_prob)\n",
    "\n",
    "    # Evaluate the model using accuracy, F1-score, and AUC\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(f\"{model_name} F1 Score: {f1}\")\n",
    "    print(f\"{model_name} AUC: {auc}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Implement ensemble method (majority voting for predictions, average for probabilities)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n",
    "\n",
    "# Average the probabilities for the ensemble AUC\n",
    "ensemble_probabilities = np.mean(ensemble_probabilities, axis=0)\n",
    "\n",
    "# Evaluate the ensemble model using accuracy, F1-score, and AUC\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred_final)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_pred_final)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_probabilities)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble F1 Score:\", ensemble_f1)\n",
    "print(\"Ensemble AUC:\", ensemble_auc)\n",
    "print(classification_report(y_test, ensemble_pred_final))\n",
    "\n",
    "with open('xgboost_tfidf_LLM_v5.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Body_Part</th>\n",
       "      <th>Location</th>\n",
       "      <th>Product_1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.688581</td>\n",
       "      <td>-0.935222</td>\n",
       "      <td>0.817527</td>\n",
       "      <td>-0.761139</td>\n",
       "      <td>-0.652828</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>0.670022</td>\n",
       "      <td>0.365266</td>\n",
       "      <td>-0.524021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044480</td>\n",
       "      <td>-1.460328</td>\n",
       "      <td>-0.969432</td>\n",
       "      <td>-0.424290</td>\n",
       "      <td>0.958737</td>\n",
       "      <td>0.771442</td>\n",
       "      <td>0.605041</td>\n",
       "      <td>1.809550</td>\n",
       "      <td>-0.886592</td>\n",
       "      <td>0.810894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127114</td>\n",
       "      <td>1.068401</td>\n",
       "      <td>1.211278</td>\n",
       "      <td>-0.761139</td>\n",
       "      <td>-1.261170</td>\n",
       "      <td>-0.067307</td>\n",
       "      <td>0.401968</td>\n",
       "      <td>-1.171033</td>\n",
       "      <td>-0.941745</td>\n",
       "      <td>1.006470</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.958180</td>\n",
       "      <td>0.542523</td>\n",
       "      <td>0.639321</td>\n",
       "      <td>-1.549186</td>\n",
       "      <td>3.117182</td>\n",
       "      <td>0.274781</td>\n",
       "      <td>1.060470</td>\n",
       "      <td>0.891147</td>\n",
       "      <td>0.856496</td>\n",
       "      <td>0.460138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.779871</td>\n",
       "      <td>-0.935222</td>\n",
       "      <td>0.467527</td>\n",
       "      <td>-0.437733</td>\n",
       "      <td>-0.264824</td>\n",
       "      <td>-2.116730</td>\n",
       "      <td>1.270491</td>\n",
       "      <td>0.623171</td>\n",
       "      <td>0.208979</td>\n",
       "      <td>-0.739689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>-0.933003</td>\n",
       "      <td>1.393133</td>\n",
       "      <td>-0.871157</td>\n",
       "      <td>0.060490</td>\n",
       "      <td>0.984578</td>\n",
       "      <td>0.372143</td>\n",
       "      <td>0.502606</td>\n",
       "      <td>-0.498860</td>\n",
       "      <td>-1.560613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.012504</td>\n",
       "      <td>1.068401</td>\n",
       "      <td>-1.413726</td>\n",
       "      <td>-0.437733</td>\n",
       "      <td>-0.264824</td>\n",
       "      <td>-0.649481</td>\n",
       "      <td>-0.746886</td>\n",
       "      <td>0.398183</td>\n",
       "      <td>1.166810</td>\n",
       "      <td>-1.296542</td>\n",
       "      <td>...</td>\n",
       "      <td>1.479026</td>\n",
       "      <td>0.052324</td>\n",
       "      <td>0.504815</td>\n",
       "      <td>-0.377094</td>\n",
       "      <td>-0.771888</td>\n",
       "      <td>-0.180674</td>\n",
       "      <td>-0.538727</td>\n",
       "      <td>0.210899</td>\n",
       "      <td>-1.125427</td>\n",
       "      <td>-0.448966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.796555</td>\n",
       "      <td>-0.935222</td>\n",
       "      <td>-1.194976</td>\n",
       "      <td>2.149511</td>\n",
       "      <td>-0.706985</td>\n",
       "      <td>-0.764863</td>\n",
       "      <td>0.125405</td>\n",
       "      <td>-0.634496</td>\n",
       "      <td>-1.264164</td>\n",
       "      <td>0.714660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080112</td>\n",
       "      <td>0.356393</td>\n",
       "      <td>-1.069071</td>\n",
       "      <td>0.654946</td>\n",
       "      <td>-0.433049</td>\n",
       "      <td>0.367173</td>\n",
       "      <td>0.967608</td>\n",
       "      <td>0.599836</td>\n",
       "      <td>0.599172</td>\n",
       "      <td>1.196029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age       Sex  Body_Part  Location  Product_1         0         1  \\\n",
       "0 -0.688581 -0.935222   0.817527 -0.761139  -0.652828  0.004202 -0.202524   \n",
       "1 -0.127114  1.068401   1.211278 -0.761139  -1.261170 -0.067307  0.401968   \n",
       "2  0.779871 -0.935222   0.467527 -0.437733  -0.264824 -2.116730  1.270491   \n",
       "3 -1.012504  1.068401  -1.413726 -0.437733  -0.264824 -0.649481 -0.746886   \n",
       "4 -0.796555 -0.935222  -1.194976  2.149511  -0.706985 -0.764863  0.125405   \n",
       "\n",
       "          2         3         4  ...       374       375       376       377  \\\n",
       "0  0.670022  0.365266 -0.524021  ...  0.044480 -1.460328 -0.969432 -0.424290   \n",
       "1 -1.171033 -0.941745  1.006470  ... -1.958180  0.542523  0.639321 -1.549186   \n",
       "2  0.623171  0.208979 -0.739689  ...  0.010165 -0.933003  1.393133 -0.871157   \n",
       "3  0.398183  1.166810 -1.296542  ...  1.479026  0.052324  0.504815 -0.377094   \n",
       "4 -0.634496 -1.264164  0.714660  ...  0.080112  0.356393 -1.069071  0.654946   \n",
       "\n",
       "        378       379       380       381       382       383  \n",
       "0  0.958737  0.771442  0.605041  1.809550 -0.886592  0.810894  \n",
       "1  3.117182  0.274781  1.060470  0.891147  0.856496  0.460138  \n",
       "2  0.060490  0.984578  0.372143  0.502606 -0.498860 -1.560613  \n",
       "3 -0.771888 -0.180674 -0.538727  0.210899 -1.125427 -0.448966  \n",
       "4 -0.433049  0.367173  0.967608  0.599836  0.599172  1.196029  \n",
       "\n",
       "[5 rows x 389 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.50122628, -1.45747622, -1.41372615, -1.36997608, -1.32622601,\n",
       "       -1.28247595, -1.23872588, -1.19497581, -1.15122575,  0.46752674,\n",
       "        0.5112768 ,  0.55502687,  0.642527  ,  0.68627707,  0.73002714,\n",
       "        0.77377721,  0.81752727,  1.03627761,  1.08002768,  1.21127788,\n",
       "        1.25502794,  1.29877801])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_body_parts = X_test_final['Body_Part'].unique()\n",
    "scaled_body_parts.sort()\n",
    "scaled_body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8180479 0.9804617\n",
      "0.8434001 0.9866623\n",
      "0.07610136 0.62177515\n",
      "0.3582114 0.96472424\n",
      "0.85777724 0.97853804\n",
      "0.5630455 0.95304084\n",
      "0.07579315 0.61503655\n",
      "0.6504902 0.9727951\n",
      "0.1762011 0.8535414\n",
      "0.5228751 0.9777255\n"
     ]
    }
   ],
   "source": [
    "for index, row in X_test_final[:10].iterrows():\n",
    "    prob = []\n",
    "    for body_part in scaled_body_parts:\n",
    "        current_sample = row.copy()\n",
    "        current_sample['Body_Part'] = body_part\n",
    "        # print(current_sample)\n",
    "        prob.append(best_model.predict_proba([current_sample])[0][0])\n",
    "    prob.sort()\n",
    "    print(prob[0], prob[-1]) # min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731040566660
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "doc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731080777947
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-08 15:43:12.205899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731080592.949434    3382 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731080593.161591    3382 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 15:43:15.142342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5552c43935d48198230c44e0114b0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e40174f661f46cb9af3966524f6ab41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/71.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8feb22460f8a400cb2d728f70590b9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210f22e97b0e4edd8ac92d9fef749852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4982e71d032a46778460b3b14df9cb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration.py:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36b494bf84648c68e79b7b78f965bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py:   0%|          | 0.00/59.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c5047a5b3d4e229e4fb78e924fa88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e13e89a4c2246efb0526e513bbdbd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0241d34561434eb1d5127a4551e4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee3138c1b9143878b84c868dd1dc75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de3a95d2828441fb97844c291db3ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca8f08e52024ff0ae4bd5fea3a4e256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "sentences = ['That is a happy person', 'That is a very happy person']\n",
    "\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120373274
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pip install -U angle-emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120548157
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from angle_emb import AnglE\n",
    "from angle_emb.utils import cosine_similarity\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "doc_vecs = angle.encode([\n",
    "    'leg',\n",
    "    'ride bike',\n",
    "    'eye'\n",
    "], normalize_embedding=True)\n",
    "\n",
    "for i, dv1 in enumerate(doc_vecs):\n",
    "    for dv2 in doc_vecs[i+1:]:\n",
    "        print(cosine_similarity(dv1, dv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120602644
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567008852958679\n",
      "0.5893767476081848\n",
      "0.4712052643299103\n"
     ]
    }
   ],
   "source": [
    "doc_vecs = angle.encode([\n",
    "    'leg',\n",
    "    'swimming in the pool',\n",
    "    'eye'\n",
    "], normalize_embedding=True)\n",
    "\n",
    "for i, dv1 in enumerate(doc_vecs):\n",
    "    for dv2 in doc_vecs[i+1:]:\n",
    "        print(cosine_similarity(dv1, dv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120803404
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "doc_vecs = angle.encode([\n",
    "    'leg',\n",
    "    'play basketball',\n",
    "    'eye'\n",
    "], normalize_embedding=True)\n",
    "\n",
    "cosine_similarity(doc_vecs[0], doc_vecs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120804962
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5214386582374573\n",
      "0.42240607738494873\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(doc_vecs[0], doc_vecs[1]))\n",
    "print(cosine_similarity(doc_vecs[1], doc_vecs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120638822
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0037621 , -0.03200618, -0.01649955, ..., -0.02925281,\n",
       "         0.00287279,  0.00504245],\n",
       "       [ 0.01475093,  0.00713948, -0.0093455 , ...,  0.00296938,\n",
       "         0.02412086, -0.00261443],\n",
       "       [-0.01934158, -0.01809479, -0.02410525, ..., -0.00193071,\n",
       "        -0.020917  ,  0.01015348]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vecs"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
