{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install pickle\n",
        "!pip install xgboost\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "#from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your data (replace with your actual data loading)\n",
        "# Assuming your data is in a CSV file named 'data.csv'\n",
        "import pandas as pd\n",
        "new_columns=pd.read_csv('azureml://subscriptions/512a781e-d15a-4734-adca-96ec827531cb/resourcegroups/xwang3306-rg/workspaces/DVA_PROJECT/datastores/workspaceblobstore/paths/UI/2024-11-05_152711_UTC/neiss_10p_sample_new_columns.csv')\n",
        "data_10P = pd.read_csv('../xwang3306/NEISS_10P_Sample.csv')\n",
        "sematic=pd.read_csv('../xwang3306/10P_sematic_bert.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.9.1)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (2024.7.24)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (4.66.4)\n\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: xgboost in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.10.1)\nRequirement already satisfied: nvidia-nccl-cu12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (2.21.5)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.23.5)\n"
        }
      ],
      "execution_count": 76,
      "metadata": {
        "gather": {
          "logged": 1731029961562
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_10p=data.sample(frac=0.1,random_state=42).reset_index(drop=True)\n",
        "#data_10p.to_csv('NEISS_10P_Sample.csv')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024847560
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=data_10P.merge(new_columns,how='inner',on='CPSC_Case_Number').merge(sematic,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
        "data.rename(columns={\"sematic_distance_bert\": \"sematic_distance\"},inplace=True)\n",
        "data.head(5)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "   Unnamed: 0  CPSC_Case_Number Treatment_Date  Age  Sex  Race Other_Race  \\\n0           0         221032332     2022-09-24   14    1     0          0   \n1           1         181109464     2018-10-30   28    1     1          0   \n2           2         210103105     2020-10-24   35    1     0          0   \n3           3         161157997     2016-11-15  214    2     0          0   \n4           4         181107411     2018-10-21    4    1     0          0   \n\n   Hispanic  Body_Part  Diagnosis  ... Stratum  PSU   Weight  Year  Month  \\\n0       0.0         34         71  ...       V   77  17.2223  2022      9   \n1       0.0         79         71  ...       V   25  17.5136  2018     10   \n2       0.0         30         53  ...       S   27  76.0369  2020     10   \n3       0.0         76         53  ...       S   48  85.2143  2016     11   \n4       0.0         92         72  ...       C   20   4.9383  2018     10   \n\n   Day                  activity_at_injury  injury_mechanism  \\\n0   24                  playing basketball              fell   \n1   30                       pick up crate              bent   \n2   24  riding on mountain bike practicing              fell   \n3   15                  coming down stairs              fell   \n4   21  playing with toy kitchen appliance            caught   \n\n         object_involved  sematic_distance  \n0             basketball          0.199045  \n1                  crate          0.368001  \n2          mountain bike          0.162670  \n3                 stairs          0.150813  \n4  toy kitchen appliance          0.274862  \n\n[5 rows x 33 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>CPSC_Case_Number</th>\n      <th>Treatment_Date</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Race</th>\n      <th>Other_Race</th>\n      <th>Hispanic</th>\n      <th>Body_Part</th>\n      <th>Diagnosis</th>\n      <th>...</th>\n      <th>Stratum</th>\n      <th>PSU</th>\n      <th>Weight</th>\n      <th>Year</th>\n      <th>Month</th>\n      <th>Day</th>\n      <th>activity_at_injury</th>\n      <th>injury_mechanism</th>\n      <th>object_involved</th>\n      <th>sematic_distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>221032332</td>\n      <td>2022-09-24</td>\n      <td>14</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>34</td>\n      <td>71</td>\n      <td>...</td>\n      <td>V</td>\n      <td>77</td>\n      <td>17.2223</td>\n      <td>2022</td>\n      <td>9</td>\n      <td>24</td>\n      <td>playing basketball</td>\n      <td>fell</td>\n      <td>basketball</td>\n      <td>0.199045</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>181109464</td>\n      <td>2018-10-30</td>\n      <td>28</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>79</td>\n      <td>71</td>\n      <td>...</td>\n      <td>V</td>\n      <td>25</td>\n      <td>17.5136</td>\n      <td>2018</td>\n      <td>10</td>\n      <td>30</td>\n      <td>pick up crate</td>\n      <td>bent</td>\n      <td>crate</td>\n      <td>0.368001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>210103105</td>\n      <td>2020-10-24</td>\n      <td>35</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>30</td>\n      <td>53</td>\n      <td>...</td>\n      <td>S</td>\n      <td>27</td>\n      <td>76.0369</td>\n      <td>2020</td>\n      <td>10</td>\n      <td>24</td>\n      <td>riding on mountain bike practicing</td>\n      <td>fell</td>\n      <td>mountain bike</td>\n      <td>0.162670</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>161157997</td>\n      <td>2016-11-15</td>\n      <td>214</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>76</td>\n      <td>53</td>\n      <td>...</td>\n      <td>S</td>\n      <td>48</td>\n      <td>85.2143</td>\n      <td>2016</td>\n      <td>11</td>\n      <td>15</td>\n      <td>coming down stairs</td>\n      <td>fell</td>\n      <td>stairs</td>\n      <td>0.150813</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>181107411</td>\n      <td>2018-10-21</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>92</td>\n      <td>72</td>\n      <td>...</td>\n      <td>C</td>\n      <td>20</td>\n      <td>4.9383</td>\n      <td>2018</td>\n      <td>10</td>\n      <td>21</td>\n      <td>playing with toy kitchen appliance</td>\n      <td>caught</td>\n      <td>toy kitchen appliance</td>\n      <td>0.274862</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 33 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024848162
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data['sematic_distance']=sematic_distance"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024848411
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data['#_prod']=np.nan\n",
        "data.loc[((data['Product_1']>0)), '#_prod'] = 1\n",
        "data.loc[((data['Product_1']>0) & (data['Product_2']>0)), '#_prod'] = 2\n",
        "data.loc[((data['Product_1']>0) & (data['Product_2']>0) & (data['Product_3']>0)), '#_prod'] = 3"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024848609
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data['Disposition_recode']=np.nan\n",
        "data.loc[((data['Disposition']==1)), 'Disposition_recode'] = 0\n",
        "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
        "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
        "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
        "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
        "#data=data[data['Disposition_recode'].notna()]\n",
        "\n",
        "data['Disposition_recode_2']=0\n",
        "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024848858
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Disposition_recode'].value_counts()\n",
        "data['Disposition_recode_2'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "0    308854\n1     37458\nName: Disposition_recode_2, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024849098
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bdpt_dict={}\n",
        "bdpt_dict[0]='INTERNAL'\n",
        "bdpt_dict[30]='SHOULDER'\n",
        "bdpt_dict[31]='UPPERTRUNK'\n",
        "bdpt_dict[32]='ELBOW'\n",
        "bdpt_dict[33]='LOWERARM'\n",
        "bdpt_dict[34]='WRIST'\n",
        "bdpt_dict[35]='KNEE'\n",
        "bdpt_dict[36]='LOWERLEG'\n",
        "bdpt_dict[37]='ANKLE'\n",
        "bdpt_dict[38]='PUBICREGION'\n",
        "bdpt_dict[75]='HEAD'\n",
        "bdpt_dict[76]='FACE'\n",
        "bdpt_dict[77]='EYEBALL'\n",
        "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
        "bdpt_dict[79]='LOWERTRUNK'\n",
        "bdpt_dict[80]='UPPERARM'\n",
        "bdpt_dict[81]='UPPERLEG'\n",
        "bdpt_dict[82]='HAND'\n",
        "bdpt_dict[83]='FOOT'\n",
        "bdpt_dict[84]='25-50% OF BODY'\n",
        "bdpt_dict[85]='ALLPARTSBODY'\n",
        "bdpt_dict[86]='OTHER(OLD)'\n",
        "bdpt_dict[87]='NOTSTATED/UNK'\n",
        "bdpt_dict[88]='MOUTH'\n",
        "bdpt_dict[89]='NECK'\n",
        "bdpt_dict[90]='LOWERARM(OLD)'\n",
        "bdpt_dict[91]='LOWERLEG(OLD)'\n",
        "bdpt_dict[92]='FINGER'\n",
        "bdpt_dict[93]='TOE'\n",
        "bdpt_dict[94]='EAR'\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024849315
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['body_string']=data['Body_Part'].map(bdpt_dict)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024849516
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Narrative_LLM']=data[\"activity_at_injury\"].astype(str) + ' '+data[\"injury_mechanism\"].astype(str)+ ' ' + data[\"object_involved\"].astype(str)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024849761
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_2=data[['CPSC_Case_Number']].head(315900)\n",
        "#data_2['sematic_distance_bert']=sematic_distance_bert\n",
        "##data_2.to_csv('10P_sematic_bert_315900.csv',index=False)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024850233
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data['sematic_distance_bert']=sematic_distance_bert\n",
        "#data_out=data[['CPSC_Case_Number','sematic_distance_bert']]\n",
        "#data_out.to_csv('10P_sematic_bert.csv',index=False)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024850443
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import spacy\n",
        "import requests\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "sematic_distance=[]\n",
        "\n",
        "for i in range(len(data)):\n",
        "    #print(i)\n",
        "    word1 = data.at[i,'body_string'] #\"cut\"\n",
        "    word2 = data.at[i,'Narrative_LLM'] #\"bike\"\n",
        "\n",
        "    # SpaCy similarity\n",
        "    doc1 = nlp(word1)\n",
        "    doc2 = nlp(word2)\n",
        "    similarity = doc1.similarity(doc2)\n",
        "    sematic_distance.append(similarity)\n",
        "'''"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "'\\nimport spacy\\nimport requests\\n\\n# Load the SpaCy model\\nnlp = spacy.load(\\'en_core_web_md\\')\\n\\nsematic_distance=[]\\n\\nfor i in range(len(data)):\\n    #print(i)\\n    word1 = data.at[i,\\'body_string\\'] #\"cut\"\\n    word2 = data.at[i,\\'Narrative_LLM\\'] #\"bike\"\\n\\n    # SpaCy similarity\\n    doc1 = nlp(word1)\\n    doc2 = nlp(word2)\\n    similarity = doc1.similarity(doc2)\\n    sematic_distance.append(similarity)\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731024850636
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_after_dx(narrative):\n",
        "  if isinstance(narrative, str):\n",
        "    parts = narrative.split(\"DX\", 1)\n",
        "    if len(parts) > 1:\n",
        "      return parts[0]\n",
        "    else:\n",
        "      return narrative  # No \"DX:\" found, return the original string\n",
        "  else:\n",
        "    return narrative  # Not a string, return as is\n",
        "\n",
        "data['Narrative'] = data['Narrative'].apply(remove_after_dx)"
      ],
      "outputs": [],
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731029974370
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "import re\n",
        "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "medical_terms = {\n",
        "    \"&\": \"and\",\n",
        "    \"***\": \"\",\n",
        "    \">>\": \"clinical diagnosis\",\n",
        "    \"@\": \"at\",\n",
        "    \"abd\": \"abdomen\",\n",
        "    \"af\": \"accidental fall\",\n",
        "    \"afib\": \"atrial fibrillation\",\n",
        "    \"aki\": \"acute kidney injury\",\n",
        "    \"am\": \"morning\",\n",
        "    \"ams\": \"altered mental status\",\n",
        "    \"bac\": \"blood alcohol content\",\n",
        "    \"bal\": \"blood alcohol level,\",\n",
        "    \"biba\": \"brought in by ambulance\",\n",
        "    \"c/o\": \"complains of\",\n",
        "    \"chi\": \"closed-head injury\",\n",
        "    \"clsd\": \"closed\",\n",
        "    \"cpk\": \"creatine phosphokinase\",\n",
        "    \"cva\": \"cerebral vascular accident\",\n",
        "    \"dx\": \"clinical diagnosis\",\n",
        "    \"ecf\": \"extended-care facility\",\n",
        "    \"er\": \"emergency room\",\n",
        "    \"etoh\": \"ethyl alcohol\",\n",
        "    \"eval\": \"evaluation\",\n",
        "    \"fd\": \"fall detected\",\n",
        "    \"fx\": \"fracture\",\n",
        "    \"fxs\": \"fractures\",\n",
        "    \"glf\": \"ground level fall\",\n",
        "    \"h/o\": \"history of\",\n",
        "    \"htn\": \"hypertension\",\n",
        "    \"hx\": \"history of\",\n",
        "    \"inj\": \"injury\",\n",
        "    \"inr\": \"international normalized ratio\",\n",
        "    \"intox\": \"intoxication\",\n",
        "    \"l\": \"left\",\n",
        "    \"loc\": \"loss of consciousness\",\n",
        "    \"lt\": \"left\",\n",
        "    \"mech\": \"mechanical\",\n",
        "    \"mult\": \"multiple\",\n",
        "    \"n.h.\": \"nursing home\",\n",
        "    \"nh\": \"nursing home\",\n",
        "    \"p/w\": \"presents with\",\n",
        "    \"pm\": \"afternoon\",\n",
        "    \"pt\": \"patient\",\n",
        "    \"pta\": \"prior to arrival\",\n",
        "    \"pts\": \"patient's\",\n",
        "    \"px\": \"physical examination\", # not \"procedure\",\n",
        "    \"r\": \"right\",\n",
        "    \"r/o\": \"rules out\",\n",
        "    \"rt\": \"right\",\n",
        "    \"s'd&f\": \"slipped and fell\",\n",
        "    \"s/p\": \"after\",\n",
        "    \"sah\": \"subarachnoid hemorrhage\",\n",
        "    \"sdh\": \"acute subdural hematoma\",\n",
        "    \"sts\": \"sit-to-stand\",\n",
        "    \"t'd&f\": \"tripped and fell\",\n",
        "    \"tr\": \"trauma\",\n",
        "    \"uti\": \"urinary tract infection\",\n",
        "    \"w/\": \"with\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"wks\": \"weeks\"\n",
        "}\n",
        "\n",
        "# cleanning\n",
        "def clean_narrative(text):\n",
        "    # lowercase everything\n",
        "    text = text.lower()\n",
        "    \n",
        "    # unglue DX\n",
        "    regex_dx = r\"([ˆ\\W]*(dx)[ˆ\\W]*)\"\n",
        "    text = re.sub(regex_dx, r\". dx: \", text)\n",
        "\n",
        "    # remove age and sex identifications\n",
        "    ## regex to capture age and sex (not perfect but captures almost all of the cases)\n",
        "    regex_age_sex = r\"(\\d+)\\s*?(yof|yf|yo\\s*female|yo\\s*f|yom|ym|yr|yo\\s*male|yo\\s*m)\"\n",
        "    age_sex_match = re.search(regex_age_sex, text)\n",
        "\n",
        "    ## format age and sex\n",
        "    if age_sex_match:\n",
        "        age = age_sex_match.group(1)\n",
        "        sex = age_sex_match.group(2)\n",
        "        \n",
        "        # probably not best practice but it works with this data\n",
        "        if \"f\" in sex:\n",
        "            #text = text.replace(age_sex_match.group(0), f\"{age} years old female\")\n",
        "            text = text.replace(age_sex_match.group(0), f\"patient\")\n",
        "        elif \"m\" in sex:\n",
        "            #text = text.replace(age_sex_match.group(0), f\"{age} years old male\")\n",
        "            text = text.replace(age_sex_match.group(0), f\"patient\")\n",
        "    \n",
        "    # translate medical terms\n",
        "    for term, replacement in medical_terms.items():\n",
        "        if term == \"@\" or term == \">>\" or term == \"&\" or term == \"***\":\n",
        "            pattern = fr\"({re.escape(term)})\"\n",
        "            text = re.sub(pattern, f\" {replacement} \", text) # force spaces around replacement\n",
        "            \n",
        "        else:\n",
        "            pattern = fr\"(?<!-)\\b({re.escape(term)})\\b(?!-)\"\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # user-friendly format\n",
        "    sentences = sent_tokenizer.tokenize(text)\n",
        "    sentences = [sent.capitalize() for sent in sentences]\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "text = '72 YOF SLIPPED AND FELL ON THE FLOOR THIS AM. DX: L-3, L-4 FRACTURE, RIGHT RIB FRACTURES X 3.'\n",
        "print(\"Original text:\", text)\n",
        "print(\"Clean text:\", clean_narrative(text))\n",
        "print()\n",
        "\n",
        "# Use all CPU cores\n",
        "with mp.Pool(mp.cpu_count()) as pool:\n",
        "    data['Narrative'] = pool.map(clean_narrative, data['Narrative'])\n",
        "    \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Original text: 72 YOF SLIPPED AND FELL ON THE FLOOR THIS AM. DX: L-3, L-4 FRACTURE, RIGHT RIB FRACTURES X 3.\nClean text: Patient slipped and fell on the floor this morning. Clinical diagnosis: l-3, l-4 fracture, right rib fractures x 3.\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        }
      ],
      "execution_count": 78,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731029984453
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Narrative']=data['Narrative'].str.upper()\n",
        "data[['Narrative']]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 72,
          "data": {
            "text/plain": "                                                Narrative\n0        S HE FALL 1 WEEK AND COMPLAINS OF   HE HAS BE...\n1       A  BENT TO PICK UP CRATE AT HOME TO WITH LOW BACK\n2            RIDING ON MOUNTAIN BIKE PRACTICING FALL DOWN\n3       14 MONTH  FE  FOR  AND   WAS COMINGDOWN STAIRS...\n4       4PLAYING WITH TOY KITCHEN APPLIANCE AND  GOT  ...\n...                                                   ...\n352046                        HELPING LOAD BICYCLE IN VAN\n352047   PUSHING DOWN FOOD IN BLEND MAKING A SMOOTHIE ...\n352048                                    FALL DOWN STEPS\n352049                         WAS RIDING A BIKE AND FALL\n352050              TRIPPED OV A BLANKET  FALL   ON FLOOR\n\n[346312 rows x 1 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Narrative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S HE FALL 1 WEEK AND COMPLAINS OF   HE HAS BE...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A  BENT TO PICK UP CRATE AT HOME TO WITH LOW BACK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RIDING ON MOUNTAIN BIKE PRACTICING FALL DOWN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14 MONTH  FE  FOR  AND   WAS COMINGDOWN STAIRS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4PLAYING WITH TOY KITCHEN APPLIANCE AND  GOT  ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>352046</th>\n      <td>HELPING LOAD BICYCLE IN VAN</td>\n    </tr>\n    <tr>\n      <th>352047</th>\n      <td>PUSHING DOWN FOOD IN BLEND MAKING A SMOOTHIE ...</td>\n    </tr>\n    <tr>\n      <th>352048</th>\n      <td>FALL DOWN STEPS</td>\n    </tr>\n    <tr>\n      <th>352049</th>\n      <td>WAS RIDING A BIKE AND FALL</td>\n    </tr>\n    <tr>\n      <th>352050</th>\n      <td>TRIPPED OV A BLANKET  FALL   ON FLOOR</td>\n    </tr>\n  </tbody>\n</table>\n<p>346312 rows × 1 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731029833080
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_list=['ANKLE', 'ARM', 'BODY_PART', 'CHEST', 'CONTUSION', 'CUT', 'EAR', 'ELBOW', 'EYE', 'FACE', 'FINGER', 'FOOT', 'FOREHEAD', 'FRACTURE', 'FX', 'HAND', 'HEAD', 'HIP', 'KNEE', 'LAC', 'LACERATION', 'LEG', 'LOC', 'LOSE', 'NECK', 'PAIN', 'SHOULDER', \n",
        "'SPRAIN', 'STRAIN', 'SWELL', 'THUMB', 'TOE', 'WRIST','ABRASION', 'ACHE', 'BREAK', 'BURN', 'CHIN', 'CUT', 'ER', 'FRACTURE', 'FX', 'HIT', 'INJURY', 'LACERATION', 'LIP', 'LOSE', 'LOC', 'MOUTH', 'NOSE', 'PAIN', 'RIB', 'SCALP', 'SPRAIN', 'STRAIN', 'SWELL', 'TOE', 'TWIST', 'WRIST']\n",
        "\n",
        "for i in replace_list:\n",
        "  data['Narrative'] = data['Narrative'].str.replace(i, '')\n",
        "  data['Narrative_LLM']= data['Narrative_LLM'].str.replace(i, '')\n",
        "\n",
        "\n",
        "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YO ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' F ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YF', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' M ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
        "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
        "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('JURED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace('URED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n",
        "data['Narrative'] = data['Narrative'].str.replace(' LT ', '')\n",
        "\n",
        "data['Narrative_LLM'] = data['Narrative_LLM'].str.replace('fell', 'fall')\n"
      ],
      "outputs": [],
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731030013172
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "revision = None  # Replace with the specific revision to ensure reproducibility if the model is updated.\n",
        "\n",
        "model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", revision=revision)\n",
        "\n",
        "data=data.reset_index(drop=True)\n",
        "gist_dict={}\n",
        "for i in range(len(data)):    \n",
        "    gist_dict[str(i)]=model.encode(data.at[i,'Narrative'], convert_to_tensor=True).numpy()\n",
        "\n",
        "X_test=pd.DataFrame(gist_dict).T\n",
        "out=pd.concat([data[['CPSC_Case_Number']],X_test],axis=1)\n",
        "out.to_csv('gist_embedding_10p.csv',index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}