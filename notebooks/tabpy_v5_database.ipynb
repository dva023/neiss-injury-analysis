{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269f3b5c-4817-4322-a446-06678cd7ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Alan\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'X_scaler_10p_v0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_scaler_10p_v0.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     16\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost_embedding_10p_v0.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[1;32m~\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_scaler_10p_v0.pkl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "with open('X_scaler_10p_v0.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "with open('xgboost_embedding_10p_v0.pkl', 'rb') as file:\n",
    "    best_model = pickle.load(file)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "revision = None  # Replace with the specific revision to ensure reproducibility if the model is updated.\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", revision=revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3442568-2779-48c5-96f7-f4aaeac7a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdpt_dict={}\n",
    "bdpt_dict[30]='SHOULDER'\n",
    "bdpt_dict[31]='UPPERTRUNK'\n",
    "bdpt_dict[32]='ELBOW'\n",
    "bdpt_dict[33]='LOWERARM'\n",
    "bdpt_dict[34]='WRIST'\n",
    "bdpt_dict[35]='KNEE'\n",
    "bdpt_dict[36]='LOWERLEG'\n",
    "bdpt_dict[37]='ANKLE'\n",
    "bdpt_dict[38]='PUBICREGION'\n",
    "bdpt_dict[75]='HEAD'\n",
    "bdpt_dict[76]='FACE'\n",
    "bdpt_dict[77]='EYEBALL'\n",
    "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
    "bdpt_dict[79]='LOWERTRUNK'\n",
    "bdpt_dict[80]='UPPERARM'\n",
    "bdpt_dict[81]='UPPERLEG'\n",
    "bdpt_dict[82]='HAND'\n",
    "bdpt_dict[83]='FOOT'\n",
    "bdpt_dict[88]='MOUTH'\n",
    "bdpt_dict[89]='NECK'\n",
    "bdpt_dict[90]='LOWERARM(OLD)'\n",
    "bdpt_dict[91]='LOWERLEG(OLD)'\n",
    "bdpt_dict[92]='FINGER'\n",
    "bdpt_dict[93]='TOE'\n",
    "bdpt_dict[94]='EAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b9255-0020-4636-99bf-207895e58e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(X):\n",
    "\n",
    "    sample_df_single_record=pd.DataFrame(X,columns=['Age', 'Sex','Location','Product_1' ,'activity_at_injury', 'object_involved','injury_mechanism','More_details'])\n",
    "\n",
    "    sample_df_single_record[\"Age\"] = pd.to_numeric(sample_df_single_record[\"Age\"])\n",
    "    sample_df_single_record[\"Sex\"] = pd.to_numeric(sample_df_single_record[\"Sex\"])\n",
    "    sample_df_single_record[\"Location\"] = pd.to_numeric(sample_df_single_record[\"Location\"])\n",
    "    sample_df_single_record[\"Product_1\"] = pd.to_numeric(sample_df_single_record[\"Product_1\"])\n",
    "\n",
    "    sample_df_single_record['Narrative']=sample_df_single_record[\"activity_at_injury\"].astype(str) + ' '+sample_df_single_record[\"injury_mechanism\"].astype(str)+ ' ' + sample_df_single_record[\"object_involved\"].astype(str)+' '+sample_df_single_record[\"More_details\"].astype(str)\n",
    "\n",
    "    Embedding_df=pd.DataFrame(model.encode(sample_df_single_record['Narrative'], convert_to_tensor=True).numpy())\n",
    "    Embedding_df.columns=Embedding_df.columns.astype(str)\n",
    "\n",
    "    sample_df_single_record_2=sample_df_single_record[[\"Age\",\"Sex\",\"Location\",\"Product_1\"]]\n",
    "\n",
    "    return Embedding_df,sample_df_single_record_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afb55f-13f4-44a8-9f22-99a9dc1dc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prediction(bdpt_dict,data_ready,scaler,best_model): #,sematic_distance_bert\n",
    "\n",
    "\n",
    "    prob_out={}\n",
    "\n",
    "    k=0\n",
    "\n",
    "    for i in bdpt_dict:\n",
    "        data_ready.at[0,'Body_Part']=i\n",
    "\n",
    "        X_test_scaled = scaler.transform(data_ready.select_dtypes(include=['number']))\n",
    "        X_test_final=pd.DataFrame(X_test_scaled, columns=data_ready.select_dtypes(include=['number']).columns)\n",
    "        y_prob = best_model.predict_proba(X_test_final)[:, 1][0]\n",
    "        #print(y_prob)\n",
    "        prob_out[str(i)]=y_prob\n",
    "        k+=1\n",
    "\n",
    "    return prob_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47505586-3b9c-471b-864c-eba463cf8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Body_parts_Prob(Age, Sex,Location,Product_1 ,activity_at_injury, object_involved,injury_mechanism,More_details):\n",
    "\n",
    "    coordinates = {\n",
    "        \"Body Part\": [\"SHOULDER\", \"UPPERTRUNK\", \"ELBOW\", \"LOWERARM\", \"WRIST\", \"KNEE\", \"LOWERLEG\", \"ANKLE\", \"PUBICREGION\", \n",
    "                      \"HEAD\", \"FACE\", \"EYEBALL\", \"UPPERTRUNK(OLD)\", \"LOWERTRUNK\", \"UPPERARM\", \"UPPERLEG\", \"HAND\", \"FOOT\", \"MOUTH\",\n",
    "                     \"NECK\", \"LOWERARM(OLD)\", \"LOWERLEG(OLD)\", \"FINGER\", \"TOE\", \"EAR\"],\n",
    "        \"ID\": [30, 31, 32, 33, 34, 35, 36, 37, 38, 75, 76, 77, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94]\n",
    "        \"X\": [185, 355, 177, 153, 135, 225, 225, 237, 250, 250, 235, 259, 0, 400, 176, 220, 125, 223, 250, 262, 0, 0, 381, 270, 222],\n",
    "        \"Y\": [400, 395, 321, 297, 262, 155, 102, 47, 268, 485, 455, 465, 0, 300, 348, 215, 245, 36, 443, 426, 0, 0, 226, 20, 458]\n",
    "    }\n",
    "    db_insert = pd.DataFrame(coordinates)\n",
    "\n",
    "    X = np.column_stack([Age, Sex,Location,Product_1 ,activity_at_injury, object_involved,injury_mechanism,More_details])\n",
    "    embedding_df,sample_df_single_record_2 = create_df(X)\n",
    "    data = {\"Body_Part\": [25],}\n",
    "    df_body= pd.DataFrame(data)\n",
    "    data_ready = pd.concat([sample_df_single_record_2[['Age','Sex']],df_body,sample_df_single_record_2[['Location','Product_1']], embedding_df], axis=1)\n",
    "    df_output = gen_prediction(bdpt_dict,data_ready,scaler,best_model)\n",
    "\n",
    "    db_insert['Probability'] = db_insert['ID'].map(df_output).fillna(0)\n",
    "\n",
    "    anon_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inl5Y2RzcWh1dW5oanp0enZ6d2ZjIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTczMTAyODM5OSwiZXhwIjoyMDQ2NjA0Mzk5fQ.aYVruf5I2zk6-LTgGKz94KSiDt00c4409NqyJagcUSc\"\n",
    "    client = BodyPart(anon_key)\n",
    "    client.delete_all()\n",
    "    client.insert_all(db_insert)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309b2bb-64ce-4b93-a7e4-04c58fb5e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BodyPart:\n",
    "    def __init__(self, anon_key: str):\n",
    "        self.base_url = 'https://yycdsqhuunhjztzvzwfc.supabase.co/rest/v1/neiss'\n",
    "        self.headers = {\n",
    "            'apikey': anon_key,\n",
    "            'Authorization': f'Bearer {anon_key}',\n",
    "            'Content-Type': 'application/json',\n",
    "            'Prefer': 'return=minimal'\n",
    "        }\n",
    "    \n",
    "    def delete_all(self):\n",
    "        response = requests.delete(\n",
    "            f'{self.base_url}?identifier=eq.1',\n",
    "            headers=self.headers\n",
    "        )\n",
    "        return response.status_code < 400\n",
    "    \n",
    "    def insert_all(self, data):\n",
    "        response = requests.post(\n",
    "            self.base_url,\n",
    "            headers=self.headers,\n",
    "            json=data\n",
    "        )\n",
    "        return response.status_code < 400\n",
    "    \n",
    "    def get_all(self):\n",
    "        response = requests.get(\n",
    "            f'{self.base_url}?select=x,y,body_part,probability',\n",
    "            headers=self.headers\n",
    "        )\n",
    "        return response.json() if response.status_code < 400 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b8161f-aa58-4a1a-87fd-dbfb0671d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpy.tabpy_tools.client import Client\n",
    "\n",
    "deploy = Client('http://tabpy.ericy.me:9004/')\n",
    "deploy.set_credentials(username='dva023', password='YL8bar-_3.jXGFet')\n",
    "deploy.deploy('tabpy_v2', Predict_Body_parts_Prob, 'tabpy v2 database test', override=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dva_kernel",
   "language": "python",
   "name": "dva_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
