{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1730504158956
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/wq/q_69h39s1mg1bnpqb1mdkd4h0000gn/T/ipykernel_83671/788242875.py:30: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../data/consolidated_cleaned_neiss_2014_2023.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your data (replace with your actual data loading)\n",
    "# Assuming your data is in a CSV file named 'data.csv'\n",
    "data = pd.read_csv('../data/consolidated_cleaned_neiss_2014_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730504160020
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['Body_Part_Group']=data['Body_Part'].copy()\n",
    "data.loc[((data['Body_Part']==84) | (data['Body_Part']==0)| (data['Body_Part']==85)), 'Body_Part_Group'] = 0 # all internal\n",
    "data.loc[((data['Body_Part']==38) | (data['Body_Part']==79)), 'Body_Part_Group'] = 79 # all internal\n",
    "data.loc[((data['Body_Part']==80) | (data['Body_Part']==30) | (data['Body_Part']==89)), 'Body_Part_Group'] = 30 # all internal\n",
    "data.loc[((data['Body_Part']==32) | (data['Body_Part']==33)), 'Body_Part_Group'] = 33 # all internal\n",
    "data.loc[((data['Body_Part']==88) | (data['Body_Part']==76) | (data['Body_Part']==77)), 'Body_Part_Group'] = 76 # all internal\n",
    "data.loc[((data['Body_Part']==81) | (data['Body_Part']==35)), 'Body_Part_Group'] = 35 # all internal\n",
    "data.loc[((data['Body_Part']==87)| (data['Body_Part']==94)), 'Body_Part_Group'] = np.nan # all internal\n",
    "data.loc[((data['Body_Part']==93) | (data['Body_Part']==83)| (data['Body_Part']==37)), 'Body_Part_Group'] = 37 # all internal\n",
    "data=data[data['Body_Part_Group'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730504160345
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body_Part_Group\n",
       "75.0    606221\n",
       "76.0    425448\n",
       "37.0    401233\n",
       "79.0    304586\n",
       "92.0    285292\n",
       "30.0    238774\n",
       "35.0    225049\n",
       "33.0    208119\n",
       "31.0    197923\n",
       "82.0    154695\n",
       "0.0     131460\n",
       "36.0    127365\n",
       "34.0    115960\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Body_Part_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730504228341
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_after_dx(narrative):\n",
    "  if isinstance(narrative, str):\n",
    "    parts = narrative.split(\"DX\", 1)\n",
    "    if len(parts) > 1:\n",
    "      return parts[0]\n",
    "    else:\n",
    "      return narrative  # No \"DX:\" found, return the original string\n",
    "  else:\n",
    "    return narrative  # Not a string, return as is\n",
    "\n",
    "\n",
    "data['Narrative'] = data['Narrative'].apply(remove_after_dx)\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
    "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730504228685
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "core_col=['Age', 'Sex', 'Race','Location','Hispanic', 'Body_Part_Group','Product_1' ,'Alcohol', 'Drug','Narrative']\n",
    "data_core=data[core_col]\n",
    "data_core_sample=data_core.sample(frac=0.1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730504693013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/eric/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eric/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "corpus = data_core_sample['Narrative'].fillna('')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_core_sample['Processed_Narrative'] = corpus.apply(preprocess_text)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix = vectorizer.fit_transform(data_core_sample['Processed_Narrative'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF features with your existing data\n",
    "data_ready = pd.concat([data_core_sample, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           lower ext swell abrasionlac sts bump chair\n",
      "1    move someone heavy box fall ured leave wristle...\n",
      "2             mom play fishhook fish hook puncture lip\n",
      "3               mom sit kid chair flip fall onto floor\n",
      "4    mom mother roomwatched sister daughter fall fr...\n",
      "Name: Processed_Narrative, dtype: object\n",
      "Index(['ankle', 'arm', 'backwards', 'ball', 'basketball', 'bathroom', 'bed',\n",
      "       'bicycle', 'bike', 'box', 'break', 'burn', 'car', 'catch', 'chair',\n",
      "       'chest', 'contusion', 'couch', 'cut', 'days', 'develop', 'dog', 'door',\n",
      "       'elbow', 'eye', 'face', 'fall', 'felt', 'finger', 'floor', 'foot',\n",
      "       'football', 'forehead', 'fracture', 'fx', 'glass', 'grind', 'hand',\n",
      "       'head', 'helmet', 'hip', 'hit', 'home', 'house', 'jump', 'knee',\n",
      "       'knife', 'lac', 'laceration', 'ladder', 'land', 'leave', 'leg', 'lift',\n",
      "       'loc', 'lose', 'low', 'lower', 'lt', 'metal', 'mom', 'neck', 'pain',\n",
      "       'play', 'pop', 'present', 'rid', 'right', 'roll', 'run', 'school',\n",
      "       'shoulder', 'shower', 'sit', 'slide', 'slip', 'soccer', 'sp', 'sprain',\n",
      "       'stairs', 'stand', 'state', 'step', 'strain', 'strike', 'sustain',\n",
      "       'swell', 'table', 'thumb', 'toe', 'trampoline', 'trip', 'try', 'twist',\n",
      "       'upper', 'ured', 'use', 'walk', 'wall', 'wrist'],\n",
      "      dtype='object')\n",
      "['ankle' 'arm' 'backwards' 'ball' 'basketball' 'bathroom' 'bed' 'bicycle'\n",
      " 'bike' 'box' 'break' 'burn' 'car' 'catch' 'chair' 'chest' 'contusion'\n",
      " 'couch' 'cut' 'days' 'develop' 'dog' 'door' 'elbow' 'eye' 'face' 'fall'\n",
      " 'felt' 'finger' 'floor' 'foot' 'football' 'forehead' 'fracture' 'fx'\n",
      " 'glass' 'grind' 'hand' 'head' 'helmet' 'hip' 'hit' 'home' 'house' 'jump'\n",
      " 'knee' 'knife' 'lac' 'laceration' 'ladder' 'land' 'leave' 'leg' 'lift'\n",
      " 'loc' 'lose' 'low' 'lower' 'lt' 'metal' 'mom' 'neck' 'pain' 'play' 'pop'\n",
      " 'present' 'rid' 'right' 'roll' 'run' 'school' 'shoulder' 'shower' 'sit'\n",
      " 'slide' 'slip' 'soccer' 'sp' 'sprain' 'stairs' 'stand' 'state' 'step'\n",
      " 'strain' 'strike' 'sustain' 'swell' 'table' 'thumb' 'toe' 'trampoline'\n",
      " 'trip' 'try' 'twist' 'upper' 'ured' 'use' 'walk' 'wall' 'wrist']\n"
     ]
    }
   ],
   "source": [
    "data_ready_v2 = tfidf_df\n",
    "\n",
    "feature_columns = [\n",
    "    'arm', 'chest', 'elbow', 'finger', 'hand', 'shoulder', 'thumb', 'wrist',\n",
    "    'ankle', 'foot', 'hip', 'knee', 'leg', 'toe',\n",
    "    'eye', 'face', 'forehead', 'head', 'neck',\n",
    "    'burn', 'cut', 'laceration', 'lac',\n",
    "    'contusion', 'fracture', 'fx', 'sprain', 'strain',\n",
    "    'loc', 'pain', 'swell',\n",
    "    'backwards', 'land', 'lift', 'roll', 'run', 'sit', 'stand', 'step', 'walk',\n",
    "    'catch', 'jump', 'play', 'slide', 'slip', 'strike', 'trip', 'twist',\n",
    "    'develop', 'felt', 'lose', 'present', 'sustain', 'try', 'use',\n",
    "    'ball', 'basketball', 'bicycle', 'bike', 'football', 'helmet', 'soccer',\n",
    "    'trampoline',\n",
    "    'bed', 'chair', 'couch', 'table',\n",
    "    'bathroom', 'door', 'floor', 'stairs', 'wall',\n",
    "    'box', 'glass', 'knife', 'ladder', 'metal'\n",
    "]\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "data_ready = pd.concat([data_core_sample, tfidf_df], axis=1)\n",
    "# Separate features (X) and target variable (y)\n",
    "# X = data_core_sample.drop(['Body_Part_Group','Narrative','Processed_Narrative'], axis=1)  # ,'Processed_Narrative'] Replace 'target_variable' with your target column name\n",
    "# y = data_core_sample['Body_Part_Group']\n",
    "X = data_ready.drop(['Body_Part_Group','Narrative','Processed_Narrative'], axis=1)  # ,'Processed_Narrative'] Replace 'target_variable' with your target column name\n",
    "y = data_ready['Body_Part_Group']\n",
    "\n",
    "data_core_sample.head(10)\n",
    "# X.sample()\n",
    "# data_ready_v2.sample()\n",
    "print(data_core_sample['Processed_Narrative'].head())\n",
    "print(tfidf_df.columns)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509585318
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "XGBoost Accuracy: 0.6801572111099747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.63      0.55      2610\n",
      "           1       0.81      0.53      0.64      4730\n",
      "           2       0.62      0.44      0.52      3875\n",
      "           3       0.62      0.59      0.60      4265\n",
      "           4       0.85      0.60      0.70      2273\n",
      "           5       0.90      0.59      0.71      4665\n",
      "           6       0.74      0.48      0.58      2555\n",
      "           7       0.82      0.83      0.83      7871\n",
      "           8       0.63      0.79      0.70     12148\n",
      "           9       0.62      0.67      0.64      8482\n",
      "          10       0.47      0.63      0.54      6161\n",
      "          11       0.78      0.76      0.77      3101\n",
      "          12       0.91      0.84      0.88      5707\n",
      "\n",
      "    accuracy                           0.68     68443\n",
      "   macro avg       0.71      0.64      0.67     68443\n",
      "weighted avg       0.70      0.68      0.68     68443\n",
      "\n",
      "Ensemble Accuracy: 0.6801572111099747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.63      0.55      2610\n",
      "           1       0.81      0.53      0.64      4730\n",
      "           2       0.62      0.44      0.52      3875\n",
      "           3       0.62      0.59      0.60      4265\n",
      "           4       0.85      0.60      0.70      2273\n",
      "           5       0.90      0.59      0.71      4665\n",
      "           6       0.74      0.48      0.58      2555\n",
      "           7       0.82      0.83      0.83      7871\n",
      "           8       0.63      0.79      0.70     12148\n",
      "           9       0.62      0.67      0.64      8482\n",
      "          10       0.47      0.63      0.54      6161\n",
      "          11       0.78      0.76      0.77      3101\n",
      "          12       0.91      0.84      0.88      5707\n",
      "\n",
      "    accuracy                           0.68     68443\n",
      "   macro avg       0.71      0.64      0.67     68443\n",
      "weighted avg       0.70      0.68      0.68     68443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "# X = data_ready.drop(['Body_Part_Group','Narrative','Processed_Narrative'], axis=1)  # ,'Processed_Narrative'] Replace 'target_variable' with your target column name\n",
    "# y = data_ready['Body_Part_Group']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a scaler for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
    "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
    "\n",
    "# Convert scaled features back to DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
    "\n",
    "# Combine scaled numerical features with categorical features\n",
    "X_train_final = X_train_scaled_df.copy()\n",
    "X_test_final = X_test_scaled_df.copy()\n",
    "\n",
    "\n",
    "# Define models and their parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    # 'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
    "    # 'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\n",
    "   # 'Lasso': (Lasso(), {'lasso__alpha': [0.1, 1.0]}),\n",
    "    #'SVM': (SVC(probability=True), {'svm__C': [0.1, 1.0], 'svm__kernel': ['linear', 'rbf']}),\n",
    "    'XGBoost': (XGBClassifier(objective='multi:softmax', num_class=len(le.classes_)),\n",
    "               {'xgboost__learning_rate': [0.1, 0.01], 'xgboost__max_depth': [3, 5]})\n",
    "}\n",
    "\n",
    "# Create an ensemble of models\n",
    "ensemble_predictions = []\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # Create a pipeline for preprocessing and model training\n",
    "    pipeline = Pipeline([\n",
    "        (model_name.lower(), model)\n",
    "    ])\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test_final)\n",
    "    ensemble_predictions.append(y_pred)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy:\", accuracy)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Implement your ensemble method (e.g., voting)\n",
    "# For example, you can use a simple majority vote:\n",
    "from scipy.stats import mode\n",
    "\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred_final)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(classification_report(y_test, ensemble_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509586942
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6906997584352841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.66      0.59      3931\n",
      "           1       0.77      0.55      0.64      7112\n",
      "           2       0.56      0.47      0.51      5754\n",
      "           3       0.60      0.60      0.60      6345\n",
      "           4       0.83      0.61      0.70      3452\n",
      "           5       0.86      0.60      0.70      6970\n",
      "           6       0.72      0.50      0.59      3762\n",
      "           7       0.79      0.84      0.81     11929\n",
      "           8       0.67      0.78      0.72     18315\n",
      "           9       0.66      0.68      0.67     12723\n",
      "          10       0.51      0.64      0.57      9137\n",
      "          11       0.76      0.76      0.76      4584\n",
      "          12       0.90      0.85      0.88      8650\n",
      "\n",
      "    accuracy                           0.69    102664\n",
      "   macro avg       0.71      0.66      0.67    102664\n",
      "weighted avg       0.70      0.69      0.69    102664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = data_ready.drop(['Body_Part_Group','Narrative','Processed_Narrative'], axis=1)  # Replace 'target_variable' with your target column name\n",
    "y = data_ready['Body_Part_Group']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=len(le.classes_),\n",
    "                     # Add other hyperparameters as needed (e.g., learning_rate, max_depth)\n",
    "                     )\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509586970
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_toy.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Replace 'your_model_file.pkl' with the actual file name\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('xgb_toy.pkl', 'rb') as f:  # Replace 'your_model_file.pkl' with the actual file name\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509586995
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def Predict_Body_parts_Prob(Age, Sex, Race,Location,Hispanic, Product_1 ,Alcohol, Drug):\n",
    "    X = np.column_stack([Age, Sex, Race,Location,Hispanic, Product_1 ,Alcohol, Drug])\n",
    "    \n",
    "    return loaded_model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587018
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Predict_Body_parts_Prob(20,1,0,1,0,1,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587039
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba = loaded_model.predict_proba(X_test)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587067
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(model, open(\"xgb_toy.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587094
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "class_labels = le.classes_\n",
    "\n",
    "# Print the class labels\n",
    "print(\"Class Labels:\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587198
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "part=[75,76,79,92,31,37,35,82,83,30,36,33,34]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587219
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for i in part:\n",
    "    data_filtered=data[data['Body_Part']==i]\n",
    "    # Assuming you want to create a word cloud from the 'Narrative' column\n",
    "    text = \" \".join(review for review in data_filtered.Narrative.astype(str))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587246
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(2014,2024):\n",
    "    data_filtered=data[data['Year']==i]\n",
    "    # Assuming you want to create a word cloud from the 'Narrative' column\n",
    "    text = \" \".join(review for review in data_filtered.Narrative.astype(str))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730509587272
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "# Assuming you want to create a word cloud from the 'Narrative' column\n",
    "text = \" \".join(review for review in data_filtered.Narrative.astype(str))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
