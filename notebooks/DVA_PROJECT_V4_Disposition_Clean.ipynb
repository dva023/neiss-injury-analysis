{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1731120966160
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:azureml.core.run:Could not load the run context. Logging offline\n",
      "INFO:azureml.core.run:Could not load the run context. Logging offline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: xgboost in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.10.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (2.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pickle\n",
    "!pip install xgboost\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your data (replace with your actual data loading)\n",
    "# Assuming your data is in a CSV file named 'data.csv'\n",
    "import pandas as pd\n",
    "new_columns=pd.read_csv('azureml://subscriptions/512a781e-d15a-4734-adca-96ec827531cb/resourcegroups/xwang3306-rg/workspaces/DVA_PROJECT/datastores/workspaceblobstore/paths/UI/2024-11-05_152711_UTC/neiss_10p_sample_new_columns.csv')\n",
    "data_10P = pd.read_csv('../xwang3306/NEISS_10P_Sample.csv')\n",
    "sematic=pd.read_csv('../xwang3306/10P_sematic_bert.csv')\n",
    "embedding=pd.read_csv('../xwang3306/gist_embedding_10p.csv')\n",
    "#embedding_2=pd.read_csv('../xwang3306/aliba_embedding_10p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731039776971
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#data_10p=data.sample(frac=0.1,random_state=42).reset_index(drop=True)\n",
    "#data_10p.to_csv('NEISS_10P_Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120993237
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CPSC_Case_Number</th>\n",
       "      <th>Treatment_Date</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Other_Race</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Body_Part</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>...</th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Stratum</th>\n",
       "      <th>PSU</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>activity_at_injury</th>\n",
       "      <th>injury_mechanism</th>\n",
       "      <th>object_involved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221032332</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>14YOM REPORTS HE FELL 1 WEEK AND COMPLAINS OF ...</td>\n",
       "      <td>V</td>\n",
       "      <td>77</td>\n",
       "      <td>17.2223</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>playing basketball</td>\n",
       "      <td>fell</td>\n",
       "      <td>basketball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>181109464</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>A 28YOM BENT TO PICK UP CRATE AT HOME TO ED WI...</td>\n",
       "      <td>V</td>\n",
       "      <td>25</td>\n",
       "      <td>17.5136</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>pick up crate</td>\n",
       "      <td>bent</td>\n",
       "      <td>crate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210103105</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>35YOMRIDING ON MOUNTAIN BIKE PRACTICING FELL D...</td>\n",
       "      <td>S</td>\n",
       "      <td>27</td>\n",
       "      <td>76.0369</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>riding on mountain bike practicing</td>\n",
       "      <td>fell</td>\n",
       "      <td>mountain bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>161157997</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...</td>\n",
       "      <td>S</td>\n",
       "      <td>48</td>\n",
       "      <td>85.2143</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>coming down stairs</td>\n",
       "      <td>fell</td>\n",
       "      <td>stairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>181107411</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...</td>\n",
       "      <td>C</td>\n",
       "      <td>20</td>\n",
       "      <td>4.9383</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>playing with toy kitchen appliance</td>\n",
       "      <td>caught</td>\n",
       "      <td>toy kitchen appliance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  CPSC_Case_Number Treatment_Date  Age  Sex  Race Other_Race  \\\n",
       "0           0         221032332     2022-09-24   14    1     0          0   \n",
       "1           1         181109464     2018-10-30   28    1     1          0   \n",
       "2           2         210103105     2020-10-24   35    1     0          0   \n",
       "3           3         161157997     2016-11-15  214    2     0          0   \n",
       "4           4         181107411     2018-10-21    4    1     0          0   \n",
       "\n",
       "   Hispanic  Body_Part  Diagnosis  ...  \\\n",
       "0       0.0         34         71  ...   \n",
       "1       0.0         79         71  ...   \n",
       "2       0.0         30         53  ...   \n",
       "3       0.0         76         53  ...   \n",
       "4       0.0         92         72  ...   \n",
       "\n",
       "                                           Narrative  Stratum PSU   Weight  \\\n",
       "0  14YOM REPORTS HE FELL 1 WEEK AND COMPLAINS OF ...        V  77  17.2223   \n",
       "1  A 28YOM BENT TO PICK UP CRATE AT HOME TO ED WI...        V  25  17.5136   \n",
       "2  35YOMRIDING ON MOUNTAIN BIKE PRACTICING FELL D...        S  27  76.0369   \n",
       "3  14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...        S  48  85.2143   \n",
       "4  4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...        C  20   4.9383   \n",
       "\n",
       "   Year  Month  Day                  activity_at_injury  injury_mechanism  \\\n",
       "0  2022      9   24                  playing basketball              fell   \n",
       "1  2018     10   30                       pick up crate              bent   \n",
       "2  2020     10   24  riding on mountain bike practicing              fell   \n",
       "3  2016     11   15                  coming down stairs              fell   \n",
       "4  2018     10   21  playing with toy kitchen appliance            caught   \n",
       "\n",
       "         object_involved  \n",
       "0             basketball  \n",
       "1                  crate  \n",
       "2          mountain bike  \n",
       "3                 stairs  \n",
       "4  toy kitchen appliance  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data_10P.merge(new_columns,how='inner',on='CPSC_Case_Number').merge(sematic,how='inner',on='CPSC_Case_Number').merge(embedding,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
    "data.rename(columns={\"sematic_distance_bert\": \"sematic_distance\"},inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121001405
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['#_prod']=np.nan\n",
    "data.loc[((data['Product_1']>0)), '#_prod'] = 1\n",
    "data.loc[((data['Product_1']>0) & (data['Product_2']>0)), '#_prod'] = 2\n",
    "data.loc[((data['Product_1']>0) & (data['Product_2']>0) & (data['Product_3']>0)), '#_prod'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121003728
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['Disposition_recode']=np.nan\n",
    "data.loc[((data['Disposition']==1)), 'Disposition_recode'] = 0\n",
    "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
    "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
    "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
    "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
    "data=data[data['Disposition_recode'].notna()]\n",
    "\n",
    "data['Disposition_recode_2']=0\n",
    "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data=data[(data['Body_Part']!=0) & (data['Body_Part']!=84) & (data['Body_Part']!=85) & (data['Body_Part']!=86) & (data['Body_Part']!=87)]\n",
    "data['Disposition_recode'].value_counts()\n",
    "data['Disposition_recode_2'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121014315
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "bdpt_dict={}\n",
    "bdpt_dict[0]='INTERNAL'\n",
    "bdpt_dict[30]='SHOULDER'\n",
    "bdpt_dict[31]='UPPERTRUNK'\n",
    "bdpt_dict[32]='ELBOW'\n",
    "bdpt_dict[33]='LOWERARM'\n",
    "bdpt_dict[34]='WRIST'\n",
    "bdpt_dict[35]='KNEE'\n",
    "bdpt_dict[36]='LOWERLEG'\n",
    "bdpt_dict[37]='ANKLE'\n",
    "bdpt_dict[38]='PUBICREGION'\n",
    "bdpt_dict[75]='HEAD'\n",
    "bdpt_dict[76]='FACE'\n",
    "bdpt_dict[77]='EYEBALL'\n",
    "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
    "bdpt_dict[79]='LOWERTRUNK'\n",
    "bdpt_dict[80]='UPPERARM'\n",
    "bdpt_dict[81]='UPPERLEG'\n",
    "bdpt_dict[82]='HAND'\n",
    "bdpt_dict[83]='FOOT'\n",
    "bdpt_dict[84]='25-50% OF BODY'\n",
    "bdpt_dict[85]='ALLPARTSBODY'\n",
    "bdpt_dict[86]='OTHER(OLD)'\n",
    "bdpt_dict[87]='NOTSTATED/UNK'\n",
    "bdpt_dict[88]='MOUTH'\n",
    "bdpt_dict[89]='NECK'\n",
    "bdpt_dict[90]='LOWERARM(OLD)'\n",
    "bdpt_dict[91]='LOWERLEG(OLD)'\n",
    "bdpt_dict[92]='FINGER'\n",
    "bdpt_dict[93]='TOE'\n",
    "bdpt_dict[94]='EAR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121020538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data['body_string']=data['Body_Part'].map(bdpt_dict)\n",
    "data['Narrative_LLM']=data[\"activity_at_injury\"].astype(str) + ' '+data[\"injury_mechanism\"].astype(str)+ ' ' + data[\"object_involved\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114157682
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_after_dx(narrative):\n",
    "  if isinstance(narrative, str):\n",
    "    parts = narrative.split(\"DX\", 1)\n",
    "    if len(parts) > 1:\n",
    "      return parts[0]\n",
    "    else:\n",
    "      return narrative  # No \"DX:\" found, return the original string\n",
    "  else:\n",
    "    return narrative  # Not a string, return as is\n",
    "\n",
    "data['Narrative'] = data['Narrative'].apply(remove_after_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114166839
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 72 YOF SLIPPED AND FELL ON THE FLOOR THIS AM. DX: L-3, L-4 FRACTURE, RIGHT RIB FRACTURES X 3.\n",
      "Clean text: Patient slipped and fell on the floor this morning. Clinical diagnosis: l-3, l-4 fracture, right rib fractures x 3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import re\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "medical_terms = {\n",
    "    \"&\": \"and\",\n",
    "    \"***\": \"\",\n",
    "    \">>\": \"clinical diagnosis\",\n",
    "    \"@\": \"at\",\n",
    "    \"abd\": \"abdomen\",\n",
    "    \"af\": \"accidental fall\",\n",
    "    \"afib\": \"atrial fibrillation\",\n",
    "    \"aki\": \"acute kidney injury\",\n",
    "    \"am\": \"morning\",\n",
    "    \"ams\": \"altered mental status\",\n",
    "    \"bac\": \"blood alcohol content\",\n",
    "    \"bal\": \"blood alcohol level,\",\n",
    "    \"biba\": \"brought in by ambulance\",\n",
    "    \"c/o\": \"complains of\",\n",
    "    \"chi\": \"closed-head injury\",\n",
    "    \"clsd\": \"closed\",\n",
    "    \"cpk\": \"creatine phosphokinase\",\n",
    "    \"cva\": \"cerebral vascular accident\",\n",
    "    \"dx\": \"clinical diagnosis\",\n",
    "    \"ecf\": \"extended-care facility\",\n",
    "    \"er\": \"emergency room\",\n",
    "    \"etoh\": \"ethyl alcohol\",\n",
    "    \"eval\": \"evaluation\",\n",
    "    \"fd\": \"fall detected\",\n",
    "    \"fx\": \"fracture\",\n",
    "    \"fxs\": \"fractures\",\n",
    "    \"glf\": \"ground level fall\",\n",
    "    \"h/o\": \"history of\",\n",
    "    \"htn\": \"hypertension\",\n",
    "    \"hx\": \"history of\",\n",
    "    \"inj\": \"injury\",\n",
    "    \"inr\": \"international normalized ratio\",\n",
    "    \"intox\": \"intoxication\",\n",
    "    \"l\": \"left\",\n",
    "    \"loc\": \"loss of consciousness\",\n",
    "    \"lt\": \"left\",\n",
    "    \"mech\": \"mechanical\",\n",
    "    \"mult\": \"multiple\",\n",
    "    \"n.h.\": \"nursing home\",\n",
    "    \"nh\": \"nursing home\",\n",
    "    \"p/w\": \"presents with\",\n",
    "    \"pm\": \"afternoon\",\n",
    "    \"pt\": \"patient\",\n",
    "    \"pta\": \"prior to arrival\",\n",
    "    \"pts\": \"patient's\",\n",
    "    \"px\": \"physical examination\", # not \"procedure\",\n",
    "    \"r\": \"right\",\n",
    "    \"r/o\": \"rules out\",\n",
    "    \"rt\": \"right\",\n",
    "    \"s'd&f\": \"slipped and fell\",\n",
    "    \"s/p\": \"after\",\n",
    "    \"sah\": \"subarachnoid hemorrhage\",\n",
    "    \"sdh\": \"acute subdural hematoma\",\n",
    "    \"sts\": \"sit-to-stand\",\n",
    "    \"t'd&f\": \"tripped and fell\",\n",
    "    \"tr\": \"trauma\",\n",
    "    \"uti\": \"urinary tract infection\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"wks\": \"weeks\"\n",
    "}\n",
    "\n",
    "# cleanning\n",
    "def clean_narrative(text):\n",
    "    # lowercase everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # unglue DX\n",
    "    regex_dx = r\"([ˆ\\W]*(dx)[ˆ\\W]*)\"\n",
    "    text = re.sub(regex_dx, r\". dx: \", text)\n",
    "\n",
    "    # remove age and sex identifications\n",
    "    ## regex to capture age and sex (not perfect but captures almost all of the cases)\n",
    "    regex_age_sex = r\"(\\d+)\\s*?(yof|yf|yo\\s*female|yo\\s*f|yom|ym|yr|yo\\s*male|yo\\s*m)\"\n",
    "    age_sex_match = re.search(regex_age_sex, text)\n",
    "\n",
    "    ## format age and sex\n",
    "    if age_sex_match:\n",
    "        age = age_sex_match.group(1)\n",
    "        sex = age_sex_match.group(2)\n",
    "        \n",
    "        # probably not best practice but it works with this data\n",
    "        if \"f\" in sex:\n",
    "            #text = text.replace(age_sex_match.group(0), f\"{age} years old female\")\n",
    "            text = text.replace(age_sex_match.group(0), f\"patient\")\n",
    "        elif \"m\" in sex:\n",
    "            #text = text.replace(age_sex_match.group(0), f\"{age} years old male\")\n",
    "            text = text.replace(age_sex_match.group(0), f\"patient\")\n",
    "    \n",
    "    # translate medical terms\n",
    "    for term, replacement in medical_terms.items():\n",
    "        if term == \"@\" or term == \">>\" or term == \"&\" or term == \"***\":\n",
    "            pattern = fr\"({re.escape(term)})\"\n",
    "            text = re.sub(pattern, f\" {replacement} \", text) # force spaces around replacement\n",
    "            \n",
    "        else:\n",
    "            pattern = fr\"(?<!-)\\b({re.escape(term)})\\b(?!-)\"\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    # user-friendly format\n",
    "    sentences = sent_tokenizer.tokenize(text)\n",
    "    sentences = [sent.capitalize() for sent in sentences]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "text = '72 YOF SLIPPED AND FELL ON THE FLOOR THIS AM. DX: L-3, L-4 FRACTURE, RIGHT RIB FRACTURES X 3.'\n",
    "print(\"Original text:\", text)\n",
    "print(\"Clean text:\", clean_narrative(text))\n",
    "print()\n",
    "\n",
    "# Use all CPU cores\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    data['Narrative'] = pool.map(clean_narrative, data['Narrative'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114169143
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PATIENT REPORTS HE FELL 1 WEEK AND COMPLAINS O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A PATIENT BENT TO PICK UP CRATE AT HOME TO ED ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATIENTRIDING ON MOUNTAIN BIKE PRACTICING FELL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242139</th>\n",
       "      <td>PATIENT FOOT PAIN SP FALL OFF BIKE TODAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242140</th>\n",
       "      <td>PATIENT LAC HEAD  ON HAMOMOCK HIT HEAD ON FENC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242141</th>\n",
       "      <td>PATIENT FROM THE NURSING HOME FELL OUT OF A WH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242142</th>\n",
       "      <td>PATIENT CO PAIN TO BOTTOM OF HER FEET 22 CALLU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242143</th>\n",
       "      <td>RIGHT HAND CONTUSION PATIENT PUNCHED A WALL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238207 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Narrative\n",
       "0       PATIENT REPORTS HE FELL 1 WEEK AND COMPLAINS O...\n",
       "1       A PATIENT BENT TO PICK UP CRATE AT HOME TO ED ...\n",
       "2       PATIENTRIDING ON MOUNTAIN BIKE PRACTICING FELL...\n",
       "3       14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...\n",
       "4       4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...\n",
       "...                                                   ...\n",
       "242139           PATIENT FOOT PAIN SP FALL OFF BIKE TODAY\n",
       "242140  PATIENT LAC HEAD  ON HAMOMOCK HIT HEAD ON FENC...\n",
       "242141  PATIENT FROM THE NURSING HOME FELL OUT OF A WH...\n",
       "242142  PATIENT CO PAIN TO BOTTOM OF HER FEET 22 CALLU...\n",
       "242143        RIGHT HAND CONTUSION PATIENT PUNCHED A WALL\n",
       "\n",
       "[238207 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Narrative']=data['Narrative'].str.upper()\n",
    "data[['Narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114199643
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "replace_list=['ANKLE', 'ARM', 'BODY_PART', 'CHEST', 'CONTUSION', 'CUT', 'EAR', 'ELBOW', 'EYE', 'FACE', 'FINGER', 'FOOT', 'FOREHEAD', 'FRACTURE', 'FX', 'HAND', 'HEAD', 'HIP', 'KNEE', 'LAC', 'LACERATION', 'LEG', 'LOC', 'LOSE', 'NECK', 'PAIN', 'SHOULDER', \n",
    "'SPRAIN', 'STRAIN', 'SWELL', 'THUMB', 'TOE', 'WRIST','ABRASION', 'ACHE', 'BREAK', 'BURN', 'CHIN', 'CUT', 'ER', 'FRACTURE', 'FX', 'HIT', 'INJURY', 'LACERATION', 'LIP', 'LOSE', 'LOC', 'MOUTH', 'NOSE', 'PAIN', 'RIB', 'SCALP', 'SPRAIN', 'STRAIN', 'SWELL', 'TOE', 'TWIST', 'WRIST']\n",
    "\n",
    "for i in replace_list:\n",
    "  data['Narrative'] = data['Narrative'].str.replace(i, '')\n",
    "  data['Narrative_LLM']= data['Narrative_LLM'].str.replace(i, '')\n",
    "\n",
    "\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' F ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' M ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
    "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('JURED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('URED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' LT ', '')\n",
    "\n",
    "data['Narrative_LLM'] = data['Narrative_LLM'].str.replace('fell', 'fall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114269508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_core=data.copy() #[core_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114245702
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191347"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "212347-21000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114277211
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# please change these hard coded numbers accordingly\n",
    "data_core=data_core.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "data_test=data_core.head(21000).reset_index(drop=True)\n",
    "data_fit=data_core.tail(191347).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114281538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_bad=data_fit[data_fit['Disposition_recode_2']==1]\n",
    "df_good=data_fit[data_fit['Disposition_recode_2']==0]\n",
    "data_good_sample=df_good.sample(frac=0.2,random_state=42).reset_index(drop=True)\n",
    "data_bad_sample=df_bad.sample(frac=0.8,random_state=42).reset_index(drop=True)\n",
    "data_core_sample=pd.concat([data_good_sample,data_bad_sample]).reset_index(drop=True)\n",
    "data_core_sample=data_core_sample.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "data_core_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114374541
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "corpus = data_core_sample['Narrative'].fillna('')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_core_sample['Processed_Narrative'] = corpus.apply(preprocess_text)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(max_features=70, stop_words='english')\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix = vectorizer.fit_transform(data_core_sample['Processed_Narrative'])\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114410607
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus_LLM = data_core_sample['Narrative_LLM'].fillna('')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_core_sample['Processed_Narrative_LLM'] = corpus_LLM.apply(preprocess_text)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer_LLM = TfidfVectorizer(max_features=70, stop_words='english')\n",
    "\n",
    "\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix_LLM = vectorizer_LLM.fit_transform(data_core_sample['Processed_Narrative_LLM'])\n",
    "\n",
    "with open('tfidf_vectorizer_LLM.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer_LLM, file)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df_LLM = pd.DataFrame(tfidf_matrix_LLM.toarray(), columns=vectorizer_LLM.get_feature_names_out())\n",
    "tfidf_df_LLM=tfidf_df_LLM.add_suffix('_LLM')\n",
    "\n",
    "# Concatenate the TF-IDF features with your existing data\n",
    "data_ready = pd.concat([data_core_sample, tfidf_df,tfidf_df_LLM], axis=1)\n",
    "#data_ready = pd.concat([data_core_sample, tfidf_df_LLM], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114451560
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "new_data=data_test.copy()\n",
    "corpus_2 = new_data['Narrative'].fillna('')\n",
    "new_data['Processed_Narrative'] = corpus_2.apply(preprocess_text)\n",
    "tfidf_new = vectorizer.transform(new_data['Processed_Narrative'])\n",
    "tfidf_df_new = pd.DataFrame(tfidf_new.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "corpus_LLM_2 = new_data['Narrative_LLM'].fillna('')\n",
    "new_data['Processed_Narrative_LLM'] = corpus_LLM_2.apply(preprocess_text)\n",
    "tfidf_LLM_new = vectorizer_LLM.transform(new_data['Processed_Narrative_LLM'])\n",
    "tfidf_df_new_LLM = pd.DataFrame(tfidf_LLM_new.toarray(), columns=vectorizer_LLM.get_feature_names_out())\n",
    "tfidf_df_new_LLM=tfidf_df_new_LLM.add_suffix('_LLM')\n",
    "\n",
    "data_ready_test = pd.concat([data_test, tfidf_df_new,tfidf_df_new_LLM], axis=1)\n",
    "#data_ready_test = pd.concat([data_test, tfidf_df_new_LLM], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114704628
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "drop_list=['Narrative','Unnamed: 0',\n",
    " 'CPSC_Case_Number',\n",
    " 'Treatment_Date','Race',\n",
    " 'Other_Race',\n",
    " 'Hispanic','Diagnosis',\n",
    " 'Other_Diagnosis',\n",
    " 'Body_Part_2',\n",
    " 'Diagnosis_2',\n",
    " 'Other_Diagnosis_2',\n",
    " 'Disposition','Fire_Involvement','Product_2',\n",
    " 'Product_3',\n",
    " 'Alcohol',\n",
    " 'Drug','Stratum',\n",
    " 'PSU',\n",
    " 'Weight',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'activity_at_injury',\n",
    " 'injury_mechanism',\n",
    " 'object_involved',\n",
    " '#_prod',\n",
    " 'Disposition_recode',\n",
    " 'Disposition_recode_2',\n",
    " 'body_string',\n",
    " 'Narrative_LLM',\n",
    " 'Processed_Narrative',\n",
    " 'Processed_Narrative_LLM']\n",
    "\n",
    "drop_list_test=['Narrative','Unnamed: 0',\n",
    " 'CPSC_Case_Number',\n",
    " 'Treatment_Date','Race',\n",
    " 'Other_Race',\n",
    " 'Hispanic','Diagnosis',\n",
    " 'Other_Diagnosis',\n",
    " 'Body_Part_2',\n",
    " 'Diagnosis_2',\n",
    " 'Other_Diagnosis_2',\n",
    " 'Disposition','Fire_Involvement','Product_2',\n",
    " 'Product_3',\n",
    " 'Alcohol',\n",
    " 'Drug','Stratum',\n",
    " 'PSU',\n",
    " 'Weight',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'activity_at_injury',\n",
    " 'injury_mechanism',\n",
    " 'object_involved',\n",
    " '#_prod',\n",
    " 'Disposition_recode',\n",
    " 'Disposition_recode_2',\n",
    " 'body_string',\n",
    " 'Narrative_LLM']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731115388172
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "XGBoost Accuracy: 0.8281904761904761\n",
      "XGBoost F1 Score: 0.45366444579043\n",
      "XGBoost AUC: 0.8577310508236893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     18778\n",
      "           1       0.34      0.67      0.45      2222\n",
      "\n",
      "    accuracy                           0.83     21000\n",
      "   macro avg       0.65      0.76      0.68     21000\n",
      "weighted avg       0.89      0.83      0.85     21000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:21] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:21] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:12:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:21] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:26] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:29] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:50] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:56] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:56] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:13:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:14:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:14:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:03] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:05] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:05] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:07] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:15:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:04] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:07] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:10] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:12] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:12] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:14] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:15] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:33] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:41] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:51] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:16:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:03] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:51] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:54] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:54] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:55] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:56] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:17:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:18:07] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:18:56] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:18:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:03] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:04] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:04] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:07] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:34] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:39] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:19:40] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:19] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:23] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:43] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:20:45] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:22:46] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Implement ensemble method (majority voting for predictions, average for probabilities)\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m ensemble_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(ensemble_predictions)\n\u001b[1;32m    102\u001b[0m ensemble_pred_final \u001b[38;5;241m=\u001b[39m mode(ensemble_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Average the probabilities for the ensemble AUC\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score\n",
    "from scipy.stats import mode\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = data_ready.drop(drop_list, axis=1) \n",
    "y = data_ready['Disposition_recode_2']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test=data_ready_test.drop(drop_list_test, axis=1)\n",
    "y_test=data_ready_test['Disposition_recode_2']\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Create a scaler for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
    "\n",
    "with open('X_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
    "\n",
    "# Convert scaled features back to DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n",
    "\n",
    "# Combine scaled numerical features with categorical features\n",
    "X_train_final = X_resampled.copy()\n",
    "X_test_final = X_test_scaled_df.copy()\n",
    "y_train_final=y_resampled.copy()\n",
    "\n",
    "\n",
    "# Define models and their parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    #'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
    "    #'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\n",
    "    'XGBoost': (XGBClassifier(objective='binary:logistic', use_label_encoder=False), \n",
    "           {'xgboost__learning_rate': [0.1, 0.01, 0.001],            # Step size shrinkage to prevent overfitting\n",
    "            'xgboost__max_depth': [3, 5, 7],                         # Maximum depth of each tree\n",
    "            #'xgboost__n_estimators': [100, 200, 300],                # Number of boosting rounds\n",
    "            #'xgboost__min_child_weight': [1, 3, 5],                  # Minimum sum of instance weight needed in a child\n",
    "            #'xgboost__subsample': [0.6, 0.8,0.9],                   # Subsample ratio of the training instances\n",
    "            #'xgboost__colsample_bytree': [0.6, 0.8, 0.9],            # Subsample ratio of columns when constructing each tree\n",
    "            #'xgboost__gamma': [0.1, 0.5, 1,3],                     # Minimum loss reduction required for a split\n",
    "            #'xgboost__reg_alpha': [0.01, 0.1,0.5,1],                    # L1 regularization term on weights\n",
    "            #'xgboost__reg_lambda': [1, 1.5, 2],                      # L2 regularization term on weights\n",
    "            'xgboost__scale_pos_weight': [1, 5, 10]\n",
    "            }),\n",
    "    #'Logistic_Regression': (LogisticRegression(solver='liblinear'), {'logistic_regression__C': [0.1, 1, 10]}),\n",
    "    #'SVM': (SVC(probability=True), {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}),\n",
    "    #'NN': (MLPClassifier(max_iter=1000), {'nn__hidden_layer_sizes': [(10,), (50,), (100,)], 'nn__activation': ['relu', 'tanh']}),\n",
    "}\n",
    "\n",
    "# Initialize list to store predictions from each model\n",
    "ensemble_predictions = []\n",
    "ensemble_probabilities = []\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # Create a pipeline for preprocessing and model training\n",
    "    pipeline = Pipeline([\n",
    "        (model_name.lower(), model)\n",
    "    ])\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV with AUC score\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='precision',n_jobs=-1)\n",
    "    grid_search.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions and predict probabilities on the test set\n",
    "    y_pred = best_model.predict(X_test_final)\n",
    "    y_prob = best_model.predict_proba(X_test_final)[:, 1]  # Probability for the positive class\n",
    "    ensemble_predictions.append(y_pred)\n",
    "    ensemble_probabilities.append(y_prob)\n",
    "\n",
    "    # Evaluate the model using accuracy, F1-score, and AUC\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(f\"{model_name} F1 Score: {f1}\")\n",
    "    print(f\"{model_name} AUC: {auc}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Implement ensemble method (majority voting for predictions, average for probabilities)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n",
    "\n",
    "# Average the probabilities for the ensemble AUC\n",
    "ensemble_probabilities = np.mean(ensemble_probabilities, axis=0)\n",
    "\n",
    "# Evaluate the ensemble model using accuracy, F1-score, and AUC\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred_final)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_pred_final)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_probabilities)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble F1 Score:\", ensemble_f1)\n",
    "print(\"Ensemble AUC:\", ensemble_auc)\n",
    "print(classification_report(y_test, ensemble_pred_final))\n",
    "\n",
    "with open('xgboost_tfidf_LLM_v5.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731040566263
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxxx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xxx' is not defined"
     ]
    }
   ],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sematic_distance_hf=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    doc_vecs = angle.encode([data.at[i,'body_string'],data.at[i,'Narrative_LLM']], normalize_embedding=True)\n",
    "    sematic_distance_hf.append(cosine_similarity(doc_vecs[0], doc_vecs[1]))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the Sentence-BERT model\n",
    "model = SentenceTransformer('paraphrase-MPNet-base-v2')  # This is a lightweight, fast model\n",
    "\n",
    "#sematic_distance_bert=[]\n",
    "\n",
    "for i in range(315900,len(data)):\n",
    "    #print(i)\n",
    "    sentence1 = data.at[i,'body_string'] #\"cut\"\n",
    "    sentence2 = data.at[i,'Narrative_LLM'] #\"bike\"\n",
    "\n",
    "    embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "    similarity = util.cos_sim(embedding1, embedding2).item()\n",
    "    sematic_distance_bert.append(similarity)\n",
    "'''\n",
    "'''\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "sematic_distance=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    #print(i)\n",
    "    word1 = data.at[i,'body_string'] #\"cut\"\n",
    "    word2 = data.at[i,'Narrative_LLM'] #\"bike\"\n",
    "\n",
    "    # SpaCy similarity\n",
    "    doc1 = nlp(word1)\n",
    "    doc2 = nlp(word2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    sematic_distance.append(similarity)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731040566660
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "doc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731080777947
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-08 15:43:12.205899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731080592.949434    3382 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731080593.161591    3382 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 15:43:15.142342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5552c43935d48198230c44e0114b0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e40174f661f46cb9af3966524f6ab41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/71.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8feb22460f8a400cb2d728f70590b9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210f22e97b0e4edd8ac92d9fef749852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4982e71d032a46778460b3b14df9cb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration.py:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36b494bf84648c68e79b7b78f965bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py:   0%|          | 0.00/59.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c5047a5b3d4e229e4fb78e924fa88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e13e89a4c2246efb0526e513bbdbd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0241d34561434eb1d5127a4551e4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee3138c1b9143878b84c868dd1dc75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de3a95d2828441fb97844c291db3ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca8f08e52024ff0ae4bd5fea3a4e256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "sentences = ['That is a happy person', 'That is a very happy person']\n",
    "\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120373274
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting angle-emb\n",
      "  Downloading angle_emb-0.5.3-py3-none-any.whl (29 kB)\n",
      "Collecting boltons\n",
      "  Downloading boltons-24.1.0-py3-none-any.whl (192 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.2/192.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting prettytable\n",
      "  Downloading prettytable-3.12.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: datasets in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from angle-emb) (2.16.1)\n",
      "Requirement already satisfied: transformers>=4.32.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from angle-emb) (4.46.2)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from angle-emb) (1.5.1)\n",
      "Requirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from angle-emb) (1.10.1)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.18.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (2024.7.24)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (3.15.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (6.0.1)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (24.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from transformers>=4.32.1->angle-emb) (0.4.4)\n",
      "Requirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from bitsandbytes->angle-emb) (2.5.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (14.0.2)\n",
      "Requirement already satisfied: xxhash in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (3.10.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (0.6)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from datasets->angle-emb) (0.3.7)\n",
      "Requirement already satisfied: psutil in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from peft->angle-emb) (5.9.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from peft->angle-emb) (0.33.0)\n",
      "Requirement already satisfied: wcwidth in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from prettytable->angle-emb) (0.2.13)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn->angle-emb) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn->angle-emb) (3.5.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from wandb->angle-emb) (3.1.43)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from wandb->angle-emb) (3.20.3)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.18.0-py2.py3-none-any.whl (317 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.5/317.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from wandb->angle-emb) (4.12.2)\n",
      "Requirement already satisfied: platformdirs in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from wandb->angle-emb) (4.3.6)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from wandb->angle-emb) (8.1.7)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from wandb->angle-emb) (68.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->angle-emb) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (24.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (2.3.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp->datasets->angle-emb) (6.0.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->angle-emb) (4.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests->transformers>=4.32.1->angle-emb) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests->transformers>=4.32.1->angle-emb) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests->transformers>=4.32.1->angle-emb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests->transformers>=4.32.1->angle-emb) (1.26.19)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (3.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->bitsandbytes->angle-emb) (12.3.1.170)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes->angle-emb) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas->datasets->angle-emb) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas->datasets->angle-emb) (2.9.0.post0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->angle-emb) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes->angle-emb) (2.1.5)\n",
      "Installing collected packages: setproctitle, sentry-sdk, prettytable, einops, docker-pycreds, boltons, wandb, bitsandbytes, peft, angle-emb\n",
      "Successfully installed angle-emb-0.5.3 bitsandbytes-0.44.1 boltons-24.1.0 docker-pycreds-0.4.0 einops-0.8.0 peft-0.13.2 prettytable-3.12.0 sentry-sdk-2.18.0 setproctitle-1.3.3 wandb-0.18.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U angle-emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120548157
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "2024-11-09 02:46:42.813068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731120403.505347    3430 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731120403.706745    3430 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 02:46:45.675043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86d91ea45d441fcaffbcee0746b1359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519fe562368d414f8f606146a6ba7dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104d9d6fca6b4ffc93b567b451bb99ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaa1506e65b435bad7df8c648da12fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef73c893eaa54da29ef14c0af7da2041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6a905692cf42648b9262ea20937c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9506208896636963\n",
      "0.34343990683555603\n",
      "0.3499281108379364\n"
     ]
    }
   ],
   "source": [
    "from angle_emb import AnglE\n",
    "from angle_emb.utils import cosine_similarity\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "doc_vecs = angle.encode([\n",
    "    'leg',\n",
    "    'ride bike',\n",
    "    'eye'\n",
    "], normalize_embedding=True)\n",
    "\n",
    "for i, dv1 in enumerate(doc_vecs):\n",
    "    for dv2 in doc_vecs[i+1:]:\n",
    "        print(cosine_similarity(dv1, dv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120602644
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567008852958679\n",
      "0.5893767476081848\n",
      "0.4712052643299103\n"
     ]
    }
   ],
   "source": [
    "doc_vecs = angle.encode([\n",
    "    'leg',\n",
    "    'swimming in the pool',\n",
    "    'eye'\n",
    "], normalize_embedding=True)\n",
    "\n",
    "for i, dv1 in enumerate(doc_vecs):\n",
    "    for dv2 in doc_vecs[i+1:]:\n",
    "        print(cosine_similarity(dv1, dv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120803404
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "doc_vecs = angle.encode([\n",
    "    'leg',\n",
    "    'play basketball',\n",
    "    'eye'\n",
    "], normalize_embedding=True)\n",
    "\n",
    "cosine_similarity(doc_vecs[0], doc_vecs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120804962
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5214386582374573\n",
      "0.42240607738494873\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(doc_vecs[0], doc_vecs[1]))\n",
    "print(cosine_similarity(doc_vecs[1], doc_vecs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120638822
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0037621 , -0.03200618, -0.01649955, ..., -0.02925281,\n",
       "         0.00287279,  0.00504245],\n",
       "       [ 0.01475093,  0.00713948, -0.0093455 , ...,  0.00296938,\n",
       "         0.02412086, -0.00261443],\n",
       "       [-0.01934158, -0.01809479, -0.02410525, ..., -0.00193071,\n",
       "        -0.020917  ,  0.01015348]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vecs"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
