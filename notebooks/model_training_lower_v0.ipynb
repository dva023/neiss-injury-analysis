{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk\n",
        "!pip install xgboost\n",
        "!pip install imblearn\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "#from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "#embedding_2=pd.read_csv('../xwang3306/aliba_embedding_10p.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.9.1)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (4.66.4)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (2024.7.24)\nRequirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: xgboost in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.10.1)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.23.5)\nRequirement already satisfied: nvidia-nccl-cu12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (2.21.5)\nRequirement already satisfied: imblearn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (0.0)\nRequirement already satisfied: imbalanced-learn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imblearn) (0.12.4)\nRequirement already satisfied: scikit-learn>=1.0.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.5.1)\nRequirement already satisfied: joblib>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\nRequirement already satisfied: numpy>=1.17.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.23.5)\nRequirement already satisfied: scipy>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.10.1)\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1731356047312
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep2ZO7eSSuvg",
        "outputId": "3c5a5bb2-ff96-43cd-a1ab-611da0331e9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your data (replace with your actual data loading)\n",
        "# Assuming your data is in a CSV file named 'data.csv'\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data_size = '10p'\n",
        "# data_size = 'full'\n",
        "version = 'v0'\n",
        "\n",
        "data_10P = pd.read_csv('NEISS_10P_Sample.csv') if data_size == '10p' else pd.read_csv('../data/consolidated_cleaned_neiss_2014_2023.csv')\n",
        "\n",
        "# data_10P = pd.read_csv('../data/neiss_10p_sample.csv')\n",
        "if data_size == '10p':\n",
        "    data_10P = data_10P.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "print(\"data_10p\", data_10P.shape)\n",
        "\n",
        "embedding=pd.read_csv(f'gist_embedding_{data_size}_{version}.csv')\n",
        "print(\"embedding\", embedding.shape)\n",
        "# print(\"new_columns\", new_columns.shape)\n",
        "print(embedding.head(10))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "data_10p (352052, 28)\nembedding (346312, 385)\n   CPSC_Case_Number         0         1         2         3         4  \\\n0         221032332 -0.041426 -0.019759  0.047550 -0.016496 -0.010686   \n1         181109464 -0.017625 -0.034500  0.065491  0.020524  0.021781   \n2         210103105 -0.056189 -0.016024  0.018050  0.011191 -0.066217   \n3         161157997 -0.039718 -0.044306  0.027221  0.031631 -0.025631   \n4         181107411 -0.030849 -0.014415  0.030544 -0.059319 -0.020998   \n5         200134239  0.000362  0.013022  0.013661 -0.009646 -0.016427   \n6         140951498 -0.044273  0.000546  0.037143 -0.014956 -0.030164   \n7         221017396 -0.032238 -0.021063  0.026084 -0.042346 -0.011767   \n8         200645623 -0.011380 -0.037507  0.041981 -0.064022 -0.016811   \n9         141040420  0.019674 -0.015743  0.015878 -0.029989  0.005483   \n\n          5         6         7         8  ...       374       375       376  \\\n0  0.015076  0.113167  0.052858 -0.000614  ...  0.034040 -0.036666  0.008797   \n1  0.042113  0.060077  0.042478 -0.014973  ...  0.024239 -0.000142  0.007268   \n2  0.081201  0.032211  0.043978 -0.006360  ...  0.060739  0.014632  0.022998   \n3  0.041920  0.052326  0.061801  0.008804  ...  0.027231 -0.016303 -0.013848   \n4  0.006898  0.063128  0.002314  0.020172  ...  0.053574 -0.069382  0.023767   \n5  0.048856  0.069432  0.033588 -0.016441  ...  0.015271 -0.032191 -0.001334   \n6  0.085621  0.046328  0.065593  0.033523  ...  0.073003 -0.003709  0.004067   \n7  0.047811  0.052396  0.093629 -0.003813  ...  0.016179 -0.031867 -0.007946   \n8  0.055067  0.034287  0.093689  0.012825  ...  0.068889 -0.028480 -0.011834   \n9  0.034219  0.048066  0.047623  0.004988  ...  0.000669  0.020707 -0.023070   \n\n        377       378       379       380       381       382       383  \n0  0.024984 -0.048995  0.002709 -0.020119 -0.058474 -0.022191  0.038710  \n1 -0.015545 -0.040856 -0.042023 -0.050380  0.008760 -0.002266  0.043031  \n2 -0.055498 -0.040177  0.024382 -0.086975 -0.045614 -0.022894 -0.013169  \n3 -0.046165 -0.034185  0.006101 -0.041465 -0.041030  0.040551  0.013185  \n4  0.044406  0.034625  0.037567 -0.024320  0.038775 -0.017813  0.001262  \n5  0.017044 -0.005507 -0.011875 -0.101678  0.020554  0.008349  0.024304  \n6 -0.014014 -0.029929  0.032725 -0.047166 -0.049677 -0.029124  0.038116  \n7 -0.036613 -0.030827  0.035943 -0.010132 -0.029518 -0.000366  0.040291  \n8  0.026125 -0.003834  0.022431 -0.116032 -0.013971 -0.105048  0.038655  \n9  0.032140 -0.013655  0.035791 -0.008253 -0.096477  0.032906  0.039108  \n\n[10 rows x 385 columns]\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TewOpoBcSuvl",
        "outputId": "ba037cfa-981e-4bcc-e13c-f0d2c13b0f68",
        "gather": {
          "logged": 1731356080975
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data=data_10P.merge(new_columns,how='inner',on='CPSC_Case_Number').merge(embedding,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
        "data_10P = data_10P[['CPSC_Case_Number', 'Age', 'Sex', 'Body_Part', 'Location', 'Product_1', 'Disposition']]\n",
        "data_2=data_10P.merge(embedding,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
        "del data_10P\n",
        "del embedding\n",
        "# data_10P.head(10)\n",
        "# embedding.head(10)\n",
        "#print(data.shape)\n",
        "data=data_2.sample(frac=1)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1731356083629
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "beScWjdtSuvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(data.columns))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['CPSC_Case_Number', 'Age', 'Sex', 'Body_Part', 'Location', 'Product_1', 'Disposition', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383']\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2MmeEXYSuvn",
        "outputId": "7bc842c4-2ee1-45ab-8a6a-70e2e5116f4b",
        "gather": {
          "logged": 1731356084072
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data['Disposition_recode']=np.nan\n",
        "data.loc[((data['Disposition']==1)), 'Disposition_recode'] = 0\n",
        "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
        "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
        "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
        "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
        "data=data[data['Disposition_recode'].notna()]\n",
        "\n",
        "data['Disposition_recode_2']=0\n",
        "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1731356084997
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Hl2DVUmBSuvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=data[(data['Body_Part']!=0) & (data['Body_Part']!=84) & (data['Body_Part']!=85) & (data['Body_Part']!=86) & (data['Body_Part']!=87)]\n",
        "data['Disposition_recode'].value_counts()\n",
        "data['Disposition_recode_2'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "0    296227\n1     32874\nName: Disposition_recode_2, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "-F6fNn4qSuvp",
        "outputId": "83487658-fa6a-4e85-c06f-6fe9f2b3f3e3",
        "gather": {
          "logged": 1731356085456
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bdpt_dict={}\n",
        "bdpt_dict[0]='INTERNAL'\n",
        "bdpt_dict[30]='SHOULDER'\n",
        "bdpt_dict[31]='UPPERTRUNK'\n",
        "bdpt_dict[32]='ELBOW'\n",
        "bdpt_dict[33]='LOWERARM'\n",
        "bdpt_dict[34]='WRIST'\n",
        "bdpt_dict[35]='KNEE'\n",
        "bdpt_dict[36]='LOWERLEG'\n",
        "bdpt_dict[37]='ANKLE'\n",
        "bdpt_dict[38]='PUBICREGION'\n",
        "bdpt_dict[75]='HEAD'\n",
        "bdpt_dict[76]='FACE'\n",
        "bdpt_dict[77]='EYEBALL'\n",
        "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
        "bdpt_dict[79]='LOWERTRUNK'\n",
        "bdpt_dict[80]='UPPERARM'\n",
        "bdpt_dict[81]='UPPERLEG'\n",
        "bdpt_dict[82]='HAND'\n",
        "bdpt_dict[83]='FOOT'\n",
        "bdpt_dict[84]='25-50% OF BODY'\n",
        "bdpt_dict[85]='ALLPARTSBODY'\n",
        "bdpt_dict[86]='OTHER(OLD)'\n",
        "bdpt_dict[87]='NOTSTATED/UNK'\n",
        "bdpt_dict[88]='MOUTH'\n",
        "bdpt_dict[89]='NECK'\n",
        "bdpt_dict[90]='LOWERARM(OLD)'\n",
        "bdpt_dict[91]='LOWERLEG(OLD)'\n",
        "bdpt_dict[92]='FINGER'\n",
        "bdpt_dict[93]='TOE'\n",
        "bdpt_dict[94]='EAR'\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1731356085889
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ElkGxOnwSuvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['body_string']=data['Body_Part'].map(bdpt_dict)\n",
        "# data['Narrative_LLM']=data[\"activity_at_injury\"].astype(str) + ' '+data[\"injury_mechanism\"].astype(str)+ ' ' + data[\"object_involved\"].astype(str)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1731356086093
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqt8qW5ISuvq",
        "outputId": "18bf6f0f-4106-4635-89d7-8a1a4dccd0ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_rows, n_columns = data.shape\n",
        "\n",
        "test_size = int(total_rows * 0.2)\n",
        "train_size = total_rows - test_size\n",
        "\n",
        "print(f\"Original dataset shape: ({total_rows}, {n_columns})\")\n",
        "print(f\"Train set size: {train_size} rows\")\n",
        "print(f\"Test set size: {test_size} rows\")\n",
        "data_sample = data.sample(frac=1,random_state=42).reset_index(drop=True)\n",
        "\n",
        "data_ready = data_sample.tail(train_size).reset_index(drop=True)\n",
        "data_ready_test = data_sample.head(test_size).reset_index(drop=True)\n",
        "del data\n",
        "\n",
        "# data_test=data_core.head(21000).reset_index(drop=True)\n",
        "# data_fit=data_core.tail(191347).reset_index(drop=True)\n",
        "# df_bad=data_fit[data_fit['Disposition_recode_2']==1]\n",
        "# df_good=data_fit[data_fit['Disposition_recode_2']==0]\n",
        "# data_good_sample=df_good.sample(frac=0.2,random_state=42).reset_index(drop=True)\n",
        "# data_bad_sample=df_bad.sample(frac=0.8,random_state=42).reset_index(drop=True)\n",
        "# data_core_sample=pd.concat([data_good_sample,data_bad_sample]).reset_index(drop=True)\n",
        "# data_core_sample=data_core_sample.sample(frac=1,random_state=42).reset_index(drop=True)\n",
        "# data_core_sample.head()\n",
        "\n",
        "# data_ready = data_core_sample\n",
        "# data_ready_test = data_test\n",
        "data_ready.columns"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Original dataset shape: (329101, 394)\nTrain set size: 263281 rows\nTest set size: 65820 rows\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "Index(['CPSC_Case_Number', 'Age', 'Sex', 'Body_Part', 'Location', 'Product_1',\n       'Disposition', '0', '1', '2',\n       ...\n       '377', '378', '379', '380', '381', '382', '383', 'Disposition_recode',\n       'Disposition_recode_2', 'body_string'],\n      dtype='object', length=394)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1731356087107
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1uj4VCWSuvq",
        "outputId": "b7ba07b3-3e93-46f8-e3e0-9fc05f59ed10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_list=[\n",
        " 'CPSC_Case_Number',\n",
        " 'Disposition',\n",
        " 'Disposition_recode',\n",
        " 'Disposition_recode_2',\n",
        " 'body_string',\n",
        "]\n",
        "\n",
        "drop_list_test = drop_list"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1731356087479
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "AWOwowyFSuvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_ready.drop(drop_list, axis=1)\n",
        "y = data_ready['Disposition_recode_2']\n",
        "\n",
        "print(list(X.columns))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['Age', 'Sex', 'Body_Part', 'Location', 'Product_1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383']\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4yRb_SMSuvs",
        "outputId": "6ca480ee-8bdd-47da-b33e-1db57aeb6c18",
        "gather": {
          "logged": 1731356087709
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qT0JfM7Oc6zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score\n",
        "from scipy.stats import mode\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import DMatrix, train\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data_ready.drop(drop_list, axis=1)\n",
        "y = data_ready['Disposition_recode_2']\n",
        "\n",
        "# Encode target variable if it's categorical\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_test=data_ready_test.drop(drop_list_test, axis=1)\n",
        "y_test=data_ready_test['Disposition_recode_2']\n",
        "y_test = le.fit_transform(y_test)\n",
        "\n",
        "# Create a scaler for numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
        "\n",
        "with open(f'X_scaler_{data_size}_{version}.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler, file)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
        "\n",
        "# Convert scaled features back to DataFrame\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n",
        "\n",
        "# Combine scaled numerical features with categorical features\n",
        "X_train_final = X_resampled.copy()\n",
        "X_test_final = X_test_scaled_df.copy()\n",
        "y_train_final=y_resampled.copy()\n",
        "\n",
        "\n",
        "# Load your dataset (ensure X and y are defined properly)\n",
        "# Assuming `X_train_final`, `X_test_final`, `y_train_final`, `y_test` are already defined\n",
        "\n",
        "# Convert datasets to DMatrix format (required for xgb.train)\n",
        "dtrain = DMatrix(data=X_train_final, label=y_train_final)\n",
        "dtest = DMatrix(data=X_test_final, label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 7,\n",
        "    'scale_pos_weight': 1\n",
        "}\n",
        "\n",
        "model = train(params, dtrain, num_boost_round=5000, evals=[(dtest, 'eval')], verbose_eval=False)\n",
        "y_prob = model.predict(dtest)\n",
        "y_pred = (y_prob > 0.5).astype(int)\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"AUC: {auc}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "with open(f'xgboost_embedding_Lower_v0.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'xxxxx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost_embedding_Lower_v0.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     72\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(model, file)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mxxxxx\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Define models and their parameter grids for hyperparameter tuning\u001b[39;00m\n\u001b[1;32m     76\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m#'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m#'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#'NN': (MLPClassifier(max_iter=1000), {'nn__hidden_layer_sizes': [(10,), (50,), (100,)], 'nn__activation': ['relu', 'tanh']}),\u001b[39;00m\n\u001b[1;32m     94\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xxxxx' is not defined"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1731356826351
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ZvGIVnJTSuvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "0.8522477140213411"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731356835469
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_final.head(5)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Wi9LdTJeSuvu",
        "gather": {
          "logged": 1731356826645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_body_parts = X_test_final['Body_Part'].unique()\n",
        "scaled_body_parts.sort()\n",
        "scaled_body_parts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-wKxkJeDSuvv",
        "gather": {
          "logged": 1731356826662
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in X_test_final[:10].iterrows():\n",
        "    prob = []\n",
        "    for body_part in scaled_body_parts:\n",
        "        current_sample = row.copy()\n",
        "        current_sample['Body_Part'] = body_part\n",
        "        # print(current_sample)\n",
        "        prob.append(best_model.predict_proba([current_sample])[0][0])\n",
        "    prob.sort()\n",
        "    print(prob[0], prob[-1]) # min max"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "HwstFuYwSuvw",
        "gather": {
          "logged": 1731356826682
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xg2S2X9mSuvw"
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}