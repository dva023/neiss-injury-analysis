{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958653811
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the Sentence-BERT model\n",
    "model = SentenceTransformer('paraphrase-MPNet-base-v2')  # This is a lightweight, fast model\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    loaded_vectorizer = pickle.load(file)\n",
    "\n",
    "with open('tfidf_vectorizer_LLM.pkl', 'rb') as file:\n",
    "    loaded_vectorizer_llm = pickle.load(file)\n",
    "\n",
    "with open('X_scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "with open('xgboost_tfidf_LLM_v2.pkl', 'rb') as file:\n",
    "    best_model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958654011
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958654196
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "bdpt_dict={}\n",
    "bdpt_dict[0]='INTERNAL'\n",
    "bdpt_dict[30]='SHOULDER'\n",
    "bdpt_dict[31]='UPPERTRUNK'\n",
    "bdpt_dict[32]='ELBOW'\n",
    "bdpt_dict[33]='LOWERARM'\n",
    "bdpt_dict[34]='WRIST'\n",
    "bdpt_dict[35]='KNEE'\n",
    "bdpt_dict[36]='LOWERLEG'\n",
    "bdpt_dict[37]='ANKLE'\n",
    "bdpt_dict[38]='PUBICREGION'\n",
    "bdpt_dict[75]='HEAD'\n",
    "bdpt_dict[76]='FACE'\n",
    "bdpt_dict[77]='EYEBALL'\n",
    "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
    "bdpt_dict[79]='LOWERTRUNK'\n",
    "bdpt_dict[80]='UPPERARM'\n",
    "bdpt_dict[81]='UPPERLEG'\n",
    "bdpt_dict[82]='HAND'\n",
    "bdpt_dict[83]='FOOT'\n",
    "bdpt_dict[84]='25-50% OF BODY'\n",
    "bdpt_dict[85]='ALLPARTSBODY'\n",
    "bdpt_dict[86]='OTHER(OLD)'\n",
    "bdpt_dict[87]='NOTSTATED/UNK'\n",
    "bdpt_dict[88]='MOUTH'\n",
    "bdpt_dict[89]='NECK'\n",
    "bdpt_dict[90]='LOWERARM(OLD)'\n",
    "bdpt_dict[91]='LOWERLEG(OLD)'\n",
    "bdpt_dict[92]='FINGER'\n",
    "bdpt_dict[93]='TOE'\n",
    "bdpt_dict[94]='EAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958654389
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_df(X):\n",
    "\n",
    "    sample_df_single_record=pd.DataFrame(X,columns=['Age', 'Sex','Location','Product_1' ,'activity_at_injury', 'object_involved','injury_mechanism','More_details'])\n",
    "    \n",
    "    sample_df_single_record[\"Age\"] = pd.to_numeric(sample_df_single_record[\"Age\"])\n",
    "    sample_df_single_record[\"Sex\"] = pd.to_numeric(sample_df_single_record[\"Sex\"])\n",
    "    sample_df_single_record[\"Location\"] = pd.to_numeric(sample_df_single_record[\"Location\"])\n",
    "    sample_df_single_record[\"Product_1\"] = pd.to_numeric(sample_df_single_record[\"Product_1\"])\n",
    "\n",
    "    sample_df_single_record['Narrative_LLM']=sample_df_single_record[\"activity_at_injury\"].astype(str) + ' '+sample_df_single_record[\"injury_mechanism\"].astype(str)+ ' ' + sample_df_single_record[\"object_involved\"].astype(str)\n",
    "    sample_df_single_record['Narrative']=sample_df_single_record[\"More_details\"]\n",
    "\n",
    "    sample_df_single_record_2=sample_df_single_record[[\"Age\",\"Sex\",\"Location\",\"Product_1\"]]\n",
    "\n",
    "    return sample_df_single_record,sample_df_single_record_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958654554
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958654716
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def gen_prediction(bdpt_dict,data_ready,scaler,best_model,sematic_distance_bert):\n",
    "\n",
    "\n",
    "    prob_out={}\n",
    "\n",
    "    k=0\n",
    "\n",
    "    for i in bdpt_dict:\n",
    "        data_ready.at[0,'Body_Part']=i\n",
    "        #print(X_test_final)\n",
    "        temp=pd.DataFrame()\n",
    "        #print(len(sematic_distance_bert))\n",
    "        #print(sematic_distance_bert[k])\n",
    "        temp['sematic_distance']=[sematic_distance_bert[k]]\n",
    "        #print(temp)\n",
    "        data_ready_2=pd.concat([temp,data_ready], axis=1)\n",
    "        #print(data_ready_2)\n",
    "\n",
    "        X_test_scaled = scaler.transform(data_ready_2.select_dtypes(include=['number']))\n",
    "        X_test_final=pd.DataFrame(X_test_scaled, columns=data_ready_2.select_dtypes(include=['number']).columns)\n",
    "        y_prob = best_model.predict_proba(X_test_final)[:, 1][0]\n",
    "        #print(y_prob)\n",
    "        prob_out[str(i)]=y_prob\n",
    "        k+=1\n",
    "    \n",
    "    return prob_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958654871
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def cal_similarity(sample_df_single_record,bdpt_dict):\n",
    "\n",
    "\n",
    "    sematic_distance_bert=[]\n",
    "\n",
    "    for key in bdpt_dict:\n",
    "        #print(i)\n",
    "        sentence1 = bdpt_dict[key] #\"cut\"\n",
    "        sentence2 = sample_df_single_record.at[0,'Narrative_LLM'] #\"bike\"\n",
    "\n",
    "        embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "        embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "        similarity = util.cos_sim(embedding1, embedding2).item()\n",
    "        sematic_distance_bert.append(similarity)\n",
    "\n",
    "    return sematic_distance_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958655025
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_narrative(sample_df_single_record,loaded_vectorizer):\n",
    "\n",
    "    corpus = sample_df_single_record['Narrative'].fillna('')\n",
    "\n",
    "    sample_df_single_record['Processed_Narrative'] = corpus.apply(preprocess_text)\n",
    "    # Use the loaded vectorizer to transform new data\n",
    "    tfidf_matrix = loaded_vectorizer.transform(sample_df_single_record['Processed_Narrative'])\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=loaded_vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958655179
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_narrative_LLM(sample_df_single_record,loaded_vectorizer_llm):\n",
    "    corpus_LLM = sample_df_single_record['Narrative_LLM'].fillna('')\n",
    "\n",
    "    sample_df_single_record['Processed_Narrative_LLM'] = corpus_LLM.apply(preprocess_text)\n",
    "    # Use the loaded vectorizer to transform new data\n",
    "    tfidf_matrix_LLM = loaded_vectorizer_llm.transform(sample_df_single_record['Processed_Narrative_LLM'])\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame\n",
    "    tfidf_df_LLM = pd.DataFrame(tfidf_matrix_LLM.toarray(), columns=loaded_vectorizer_llm.get_feature_names_out())\n",
    "    tfidf_df_LLM=tfidf_df_LLM.add_suffix('_LLM')\n",
    "\n",
    "    return tfidf_df_LLM\n",
    "\n",
    "\n",
    "# Concatenate the TF-IDF features with your existing data\n",
    "\n",
    "#data_ready = pd.concat([data_core_sample, tfidf_df_LLM], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958655335
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def Predict_Body_parts_Prob(Age, Sex,Location,Product_1 ,activity_at_injury, object_involved,injury_mechanism,More_details):\n",
    "    \n",
    "    X = np.column_stack([Age, Sex,Location,Product_1 ,activity_at_injury, object_involved,injury_mechanism,More_details])\n",
    "\n",
    "    sample_df_single_record,sample_df_single_record_2 = create_df(X)\n",
    "\n",
    "    sematic_distance_bert=cal_similarity(sample_df_single_record,bdpt_dict)\n",
    "\n",
    "    tfidf_df = tfidf_narrative(sample_df_single_record,loaded_vectorizer)\n",
    "\n",
    "    tfidf_df_LLM = tfidf_narrative_LLM(sample_df_single_record,loaded_vectorizer_llm)\n",
    "\n",
    "    data = {\"Body_Part\": [25],}\n",
    "    df_body= pd.DataFrame(data)\n",
    "\n",
    "    data_ready = pd.concat([sample_df_single_record_2[['Age','Sex','Location']],df_body,sample_df_single_record_2[['Product_1']], tfidf_df,tfidf_df_LLM], axis=1)\n",
    "\n",
    "    output = gen_prediction(bdpt_dict,data_ready,scaler,best_model,sematic_distance_bert)\n",
    "    df_output = pd.DataFrame(list(output.items()), columns=['Key', 'Value'])\n",
    "    # df_output.to_csv(\"dva_v4.csv\", index=False)\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1730958636231
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.760949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.149621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>0.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0.414058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>0.401258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>0.251715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>0.259538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36</td>\n",
       "      <td>0.487792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>0.290905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38</td>\n",
       "      <td>0.386126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>75</td>\n",
       "      <td>0.377603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>76</td>\n",
       "      <td>0.363365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>77</td>\n",
       "      <td>0.348630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>78</td>\n",
       "      <td>0.468081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>79</td>\n",
       "      <td>0.427233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80</td>\n",
       "      <td>0.621083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>81</td>\n",
       "      <td>0.703228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>82</td>\n",
       "      <td>0.220103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>83</td>\n",
       "      <td>0.215032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>84</td>\n",
       "      <td>0.686952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>85</td>\n",
       "      <td>0.681222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>86</td>\n",
       "      <td>0.681222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>87</td>\n",
       "      <td>0.578372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>88</td>\n",
       "      <td>0.265228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>89</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>90</td>\n",
       "      <td>0.416818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>91</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>92</td>\n",
       "      <td>0.208740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>93</td>\n",
       "      <td>0.209099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>94</td>\n",
       "      <td>0.195597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Key     Value\n",
       "0    0  0.760949\n",
       "1   30  0.149621\n",
       "2   31  0.468800\n",
       "3   32  0.414058\n",
       "4   33  0.401258\n",
       "5   34  0.251715\n",
       "6   35  0.259538\n",
       "7   36  0.487792\n",
       "8   37  0.290905\n",
       "9   38  0.386126\n",
       "10  75  0.377603\n",
       "11  76  0.363365\n",
       "12  77  0.348630\n",
       "13  78  0.468081\n",
       "14  79  0.427233\n",
       "15  80  0.621083\n",
       "16  81  0.703228\n",
       "17  82  0.220103\n",
       "18  83  0.215032\n",
       "19  84  0.686952\n",
       "20  85  0.681222\n",
       "21  86  0.681222\n",
       "22  87  0.578372\n",
       "23  88  0.265228\n",
       "24  89  0.418100\n",
       "25  90  0.416818\n",
       "26  91  0.418100\n",
       "27  92  0.208740\n",
       "28  93  0.209099\n",
       "29  94  0.195597"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict_Body_parts_Prob(25, 1,1,3258 ,\"fall walk\", \"bike\",\"\",\"fall from bike when riding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpy.tabpy_tools.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing file \"/home/eric/workspace/neiss/tabpy_server/staging\\endpoints\\dva_v4\\1\" when saving query object\n",
      "Error with server response. code=500; text={\"message\": \"error adding endpoint\", \"info\": \"FileNotFoundError : [Errno 2] No such file or directory: '/home/eric/workspace/neiss/tabpy_server/staging\\\\\\\\endpoints\\\\\\\\dva_v4\\\\\\\\1'\"}\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "(500) error adding endpoint FileNotFoundError : [Errno 2] No such file or directory: '/home/eric/workspace/neiss/tabpy_server/staging\\\\endpoints\\\\dva_v4\\\\1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://tabpy.ericy.me:9004/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m client\u001b[38;5;241m.\u001b[39mset_credentials(username\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdva023\u001b[39m\u001b[38;5;124m'\u001b[39m, password\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYL8bar-_3.jXGFet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdva_v4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPredict_Body_parts_Prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrediction Model v4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# client.deploy('dva_v4', Predict_Body_parts_Prob, 'Prediction Model v4', override=True, path='/endpoints/dva_v4/1')\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\tabpy\\tabpy_tools\\client.py:252\u001b[0m, in \u001b[0;36mClient.deploy\u001b[1;34m(self, name, obj, description, schema, override, is_public)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_endpoint(obj)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service\u001b[38;5;241m.\u001b[39mset_endpoint(Endpoint(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobj), should_update_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\tabpy\\tabpy_tools\\rest_client.py:205\u001b[0m, in \u001b[0;36mRESTServiceClient.add_endpoint\u001b[1;34m(self, endpoint)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_endpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint):\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Adds an endpoint through the management API.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    endpoint : Endpoint\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\tabpy\\tabpy_tools\\rest.py:199\u001b[0m, in \u001b[0;36mServiceClient.POST\u001b[1;34m(self, url, data, timeout)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepends self.endpoint to the url and issues a POST request.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\tabpy\\tabpy_tools\\rest.py:110\u001b[0m, in \u001b[0;36mRequestsNetworkWrapper.POST\u001b[1;34m(self, url, data, timeout)\u001b[0m\n\u001b[0;32m    101\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    102\u001b[0m     url,\n\u001b[0;32m    103\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m):\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\Desktop\\dva_project_test\\tabpy_venv\\lib\\site-packages\\tabpy\\tabpy_tools\\rest.py:62\u001b[0m, in \u001b[0;36mRequestsNetworkWrapper.raise_error\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(response):\n\u001b[0;32m     57\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with server response. code=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     )\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(response)\n",
      "\u001b[1;31mResponseError\u001b[0m: (500) error adding endpoint FileNotFoundError : [Errno 2] No such file or directory: '/home/eric/workspace/neiss/tabpy_server/staging\\\\endpoints\\\\dva_v4\\\\1'"
     ]
    }
   ],
   "source": [
    "client = Client('http://tabpy.ericy.me:8888/')\n",
    "\n",
    "client.set_credentials(username='dva023', password='YL8bar-_3.jXGFet')\n",
    "client.deploy('dva_v4', Predict_Body_parts_Prob, 'Prediction Model v4', override=True)\n",
    "# client.deploy('dva_v4', Predict_Body_parts_Prob, 'Prediction Model v4', override=True, path='/endpoints/dva_v4/1')\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "dva_kernel",
   "language": "python",
   "name": "dva_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
