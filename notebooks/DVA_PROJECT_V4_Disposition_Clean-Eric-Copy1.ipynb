{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1731120966160
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: xgboost in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from xgboost) (2.1.2)\n",
      "Requirement already satisfied: scipy in /Users/eric/workspace/omscs/neiss-injury-analysis/.venv/lib/python3.11/site-packages (from xgboost) (1.14.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eric/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pickle\n",
    "!pip install xgboost\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your data (replace with your actual data loading)\n",
    "# Assuming your data is in a CSV file named 'data.csv'\n",
    "import pandas as pd\n",
    "new_columns=pd.read_csv('../data/neiss_10p_sample_new_columns.csv')\n",
    "data_10P = pd.read_csv('../data/NEISS_10P_Sample.csv')\n",
    "# sematic=pd.read_csv('../data/10P_sematic_bert.csv')\n",
    "embedding=pd.read_csv('../data/gist_embedding_10p.csv')\n",
    "#embedding_2=pd.read_csv('../xwang3306/aliba_embedding_10p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731120993237
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CPSC_Case_Number</th>\n",
       "      <th>Treatment_Date</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Other_Race</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Body_Part</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221032332</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034040</td>\n",
       "      <td>-0.036666</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>-0.048995</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>-0.020119</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>-0.022191</td>\n",
       "      <td>0.038710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>181109464</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.015545</td>\n",
       "      <td>-0.040856</td>\n",
       "      <td>-0.042023</td>\n",
       "      <td>-0.050380</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>-0.002266</td>\n",
       "      <td>0.043031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210103105</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060739</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.022998</td>\n",
       "      <td>-0.055498</td>\n",
       "      <td>-0.040177</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>-0.086975</td>\n",
       "      <td>-0.045614</td>\n",
       "      <td>-0.022894</td>\n",
       "      <td>-0.013169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>161157997</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027231</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>-0.013848</td>\n",
       "      <td>-0.046165</td>\n",
       "      <td>-0.034185</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>-0.041465</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>0.013185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>181107411</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053574</td>\n",
       "      <td>-0.069382</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>0.037567</td>\n",
       "      <td>-0.024320</td>\n",
       "      <td>0.038775</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>0.001262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  CPSC_Case_Number Treatment_Date  Age  Sex  Race Other_Race  \\\n",
       "0           0         221032332     2022-09-24   14    1     0          0   \n",
       "1           1         181109464     2018-10-30   28    1     1          0   \n",
       "2           2         210103105     2020-10-24   35    1     0          0   \n",
       "3           3         161157997     2016-11-15  214    2     0          0   \n",
       "4           4         181107411     2018-10-21    4    1     0          0   \n",
       "\n",
       "   Hispanic  Body_Part  Diagnosis  ...       374       375       376  \\\n",
       "0       0.0         34         71  ...  0.034040 -0.036666  0.008797   \n",
       "1       0.0         79         71  ...  0.024239 -0.000142  0.007268   \n",
       "2       0.0         30         53  ...  0.060739  0.014632  0.022998   \n",
       "3       0.0         76         53  ...  0.027231 -0.016303 -0.013848   \n",
       "4       0.0         92         72  ...  0.053574 -0.069382  0.023767   \n",
       "\n",
       "        377       378       379       380       381       382       383  \n",
       "0  0.024984 -0.048995  0.002709 -0.020119 -0.058474 -0.022191  0.038710  \n",
       "1 -0.015545 -0.040856 -0.042023 -0.050380  0.008760 -0.002266  0.043031  \n",
       "2 -0.055498 -0.040177  0.024382 -0.086975 -0.045614 -0.022894 -0.013169  \n",
       "3 -0.046165 -0.034185  0.006101 -0.041465 -0.041030  0.040551  0.013185  \n",
       "4  0.044406  0.034625  0.037567 -0.024320  0.038775 -0.017813  0.001262  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data_10P.merge(new_columns,how='inner',on='CPSC_Case_Number').merge(embedding,how='inner',on='CPSC_Case_Number').reset_index(drop=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'CPSC_Case_Number', 'Treatment_Date', 'Age', 'Sex',\n",
       "       'Race', 'Other_Race', 'Hispanic', 'Body_Part', 'Diagnosis',\n",
       "       ...\n",
       "       '374', '375', '376', '377', '378', '379', '380', '381', '382', '383'],\n",
       "      dtype='object', length=416)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121003728
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['Disposition_recode']=np.nan\n",
    "data.loc[((data['Disposition']==1)), 'Disposition_recode'] = 0\n",
    "data.loc[((data['Disposition']==2)), 'Disposition_recode'] = 1\n",
    "data.loc[((data['Disposition']==4)), 'Disposition_recode'] = 2\n",
    "data.loc[((data['Disposition']==5)), 'Disposition_recode'] = 3\n",
    "data.loc[((data['Disposition']==8)), 'Disposition_recode'] = 4\n",
    "data=data[data['Disposition_recode'].notna()]\n",
    "\n",
    "data['Disposition_recode_2']=0\n",
    "data.loc[((data['Disposition_recode']>0)), 'Disposition_recode_2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disposition_recode_2\n",
       "0    296227\n",
       "1     32874\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[(data['Body_Part']!=0) & (data['Body_Part']!=84) & (data['Body_Part']!=85) & (data['Body_Part']!=86) & (data['Body_Part']!=87)]\n",
    "data['Disposition_recode'].value_counts()\n",
    "data['Disposition_recode_2'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121014315
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "bdpt_dict={}\n",
    "bdpt_dict[0]='INTERNAL'\n",
    "bdpt_dict[30]='SHOULDER'\n",
    "bdpt_dict[31]='UPPERTRUNK'\n",
    "bdpt_dict[32]='ELBOW'\n",
    "bdpt_dict[33]='LOWERARM'\n",
    "bdpt_dict[34]='WRIST'\n",
    "bdpt_dict[35]='KNEE'\n",
    "bdpt_dict[36]='LOWERLEG'\n",
    "bdpt_dict[37]='ANKLE'\n",
    "bdpt_dict[38]='PUBICREGION'\n",
    "bdpt_dict[75]='HEAD'\n",
    "bdpt_dict[76]='FACE'\n",
    "bdpt_dict[77]='EYEBALL'\n",
    "bdpt_dict[78]='UPPERTRUNK(OLD)'\n",
    "bdpt_dict[79]='LOWERTRUNK'\n",
    "bdpt_dict[80]='UPPERARM'\n",
    "bdpt_dict[81]='UPPERLEG'\n",
    "bdpt_dict[82]='HAND'\n",
    "bdpt_dict[83]='FOOT'\n",
    "bdpt_dict[84]='25-50% OF BODY'\n",
    "bdpt_dict[85]='ALLPARTSBODY'\n",
    "bdpt_dict[86]='OTHER(OLD)'\n",
    "bdpt_dict[87]='NOTSTATED/UNK'\n",
    "bdpt_dict[88]='MOUTH'\n",
    "bdpt_dict[89]='NECK'\n",
    "bdpt_dict[90]='LOWERARM(OLD)'\n",
    "bdpt_dict[91]='LOWERLEG(OLD)'\n",
    "bdpt_dict[92]='FINGER'\n",
    "bdpt_dict[93]='TOE'\n",
    "bdpt_dict[94]='EAR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731121020538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data['body_string']=data['Body_Part'].map(bdpt_dict)\n",
    "data['Narrative_LLM']=data[\"activity_at_injury\"].astype(str) + ' '+data[\"injury_mechanism\"].astype(str)+ ' ' + data[\"object_involved\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114169143
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14YOM REPORTS HE FELL 1 WEEK AND COMPLAINS OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 28YOM BENT TO PICK UP CRATE AT HOME TO ED WI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35YOMRIDING ON MOUNTAIN BIKE PRACTICING FELL D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346307</th>\n",
       "      <td>11 YOM HELPING LOAD BICYCLE IN VAN CUT HEAD DX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346308</th>\n",
       "      <td>47YOF PUSHING DOWN FOOD IN BLENDER MAKING A SM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346309</th>\n",
       "      <td>2 YO MALE FELL DOWN STEPS DX HEAD INJURY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346310</th>\n",
       "      <td>38YOM WAS RIDING A BIKE AND FELL WRIST INJURY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346311</th>\n",
       "      <td>62YOF TRIPPED OVER A BLANKET  FELL HIT HEAD ON...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Narrative\n",
       "0       14YOM REPORTS HE FELL 1 WEEK AND COMPLAINS OF ...\n",
       "1       A 28YOM BENT TO PICK UP CRATE AT HOME TO ED WI...\n",
       "2       35YOMRIDING ON MOUNTAIN BIKE PRACTICING FELL D...\n",
       "3       14 MONTH OLD FEMALE ABRASION FOR NOSE AND FORE...\n",
       "4       4YR M PLAYING WITH TOY KITCHEN APPLIANCE AND  ...\n",
       "...                                                   ...\n",
       "346307  11 YOM HELPING LOAD BICYCLE IN VAN CUT HEAD DX...\n",
       "346308  47YOF PUSHING DOWN FOOD IN BLENDER MAKING A SM...\n",
       "346309           2 YO MALE FELL DOWN STEPS DX HEAD INJURY\n",
       "346310      38YOM WAS RIDING A BIKE AND FELL WRIST INJURY\n",
       "346311  62YOF TRIPPED OVER A BLANKET  FELL HIT HEAD ON...\n",
       "\n",
       "[329101 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['Narrative']=data['Narrative'].str.upper()\n",
    "data[['Narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114199643
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "replace_list=['ANKLE', 'ARM', 'BODY_PART', 'CHEST', 'CONTUSION', 'CUT', 'EAR', 'ELBOW', 'EYE', 'FACE', 'FINGER', 'FOOT', 'FOREHEAD', 'FRACTURE', 'FX', 'HAND', 'HEAD', 'HIP', 'KNEE', 'LAC', 'LACERATION', 'LEG', 'LOC', 'LOSE', 'NECK', 'PAIN', 'SHOULDER', \n",
    "'SPRAIN', 'STRAIN', 'SWELL', 'THUMB', 'TOE', 'WRIST','ABRASION', 'ACHE', 'BREAK', 'BURN', 'CHIN', 'CUT', 'ER', 'FRACTURE', 'FX', 'HIT', 'INJURY', 'LACERATION', 'LIP', 'LOSE', 'LOC', 'MOUTH', 'NOSE', 'PAIN', 'RIB', 'SCALP', 'SPRAIN', 'STRAIN', 'SWELL', 'TOE', 'TWIST', 'WRIST']\n",
    "\n",
    "for i in replace_list:\n",
    "  data['Narrative'] = data['Narrative'].str.replace(i, '')\n",
    "  data['Narrative_LLM']= data['Narrative_LLM'].str.replace(i, '')\n",
    "\n",
    "\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOM', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YOF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YR', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('OLD', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('MALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FEMALE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YO ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' F ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YF', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' M ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('ACCIDENTALLY','')\n",
    "data['Narrative'] = data['Narrative'].str.replace('AGO', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('TODAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('YESTERDAY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('PATIENT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' PT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURY', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('REPORT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('HURT', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('FELL', 'FALL')\n",
    "data['Narrative'] = data['Narrative'].str.replace('INJURE', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('JURED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace('URED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' ED', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' RT ', '')\n",
    "data['Narrative'] = data['Narrative'].str.replace(' LT ', '')\n",
    "\n",
    "data['Narrative_LLM'] = data['Narrative_LLM'].str.replace('fell', 'fall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114269508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_core=data.copy() #[core_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114277211
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# please change these hard coded numbers accordingly\n",
    "data_core=data_core.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "data_test=data_core.head(21000).reset_index(drop=True)\n",
    "data_fit=data_core.tail(191347).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'CPSC_Case_Number', 'Treatment_Date', 'Age', 'Sex',\n",
       "       'Race', 'Other_Race', 'Hispanic', 'Body_Part', 'Diagnosis',\n",
       "       ...\n",
       "       '378', '379', '380', '381', '382', '383', 'Disposition_recode',\n",
       "       'Disposition_recode_2', 'body_string', 'Narrative_LLM'],\n",
       "      dtype='object', length=420)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ready = data_core\n",
    "data_ready_test = data_test\n",
    "data_ready.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731114704628
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "drop_list=['Narrative','Unnamed: 0',\n",
    " 'CPSC_Case_Number',\n",
    " 'Treatment_Date','Race',\n",
    " 'Other_Race',\n",
    " 'Hispanic','Diagnosis',\n",
    " 'Other_Diagnosis',\n",
    " 'Body_Part_2',\n",
    " 'Diagnosis_2',\n",
    " 'Other_Diagnosis_2',\n",
    " 'Disposition','Fire_Involvement','Product_2',\n",
    " 'Product_3',\n",
    " 'Alcohol',\n",
    " 'Drug','Stratum',\n",
    " 'PSU',\n",
    " 'Weight',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'activity_at_injury',\n",
    " 'injury_mechanism',\n",
    " 'object_involved',\n",
    " # '#_prod',\n",
    " 'Disposition_recode',\n",
    " 'Disposition_recode_2',\n",
    " 'body_string',\n",
    " 'Narrative_LLM',\n",
    " # 'Processed_Narrative',\n",
    " # 'Processed_Narrative_LLM'\n",
    "          ]\n",
    "\n",
    "drop_list_test=['Narrative','Unnamed: 0',\n",
    " 'CPSC_Case_Number',\n",
    " 'Treatment_Date','Race',\n",
    " 'Other_Race',\n",
    " 'Hispanic','Diagnosis',\n",
    " 'Other_Diagnosis',\n",
    " 'Body_Part_2',\n",
    " 'Diagnosis_2',\n",
    " 'Other_Diagnosis_2',\n",
    " 'Disposition','Fire_Involvement','Product_2',\n",
    " 'Product_3',\n",
    " 'Alcohol',\n",
    " 'Drug','Stratum',\n",
    " 'PSU',\n",
    " 'Weight',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'activity_at_injury',\n",
    " 'injury_mechanism',\n",
    " 'object_involved',\n",
    " # '#_prod',\n",
    " 'Disposition_recode',\n",
    " 'Disposition_recode_2',\n",
    " 'body_string',\n",
    " # 'Narrative_LLM'\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Sex', 'Body_Part', 'Location', 'Product_1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383']\n"
     ]
    }
   ],
   "source": [
    "X = data_ready.drop(drop_list, axis=1) \n",
    "y = data_ready['Disposition_recode_2']\n",
    "\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1731115388172
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score\n",
    "from scipy.stats import mode\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = data_ready.drop(drop_list, axis=1) \n",
    "y = data_ready['Disposition_recode_2']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test=data_ready_test.drop(drop_list_test, axis=1)\n",
    "y_test=data_ready_test['Disposition_recode_2']\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Create a scaler for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['number']))\n",
    "\n",
    "with open('X_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test.select_dtypes(include=['number']))\n",
    "\n",
    "# Convert scaled features back to DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=['number']).columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=['number']).columns)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n",
    "\n",
    "# Combine scaled numerical features with categorical features\n",
    "X_train_final = X_resampled.copy()\n",
    "X_test_final = X_test_scaled_df.copy()\n",
    "y_train_final=y_resampled.copy()\n",
    "\n",
    "\n",
    "# Define models and their parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    #'KNN': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
    "    #'Random_Forest': (RandomForestClassifier(), {'random_forest__n_estimators': [100, 200], 'random_forest__max_depth': [4, 8]}),\n",
    "    'XGBoost': (XGBClassifier(objective='binary:logistic'), \n",
    "           {'xgboost__learning_rate': [0.1, 0.01, 0.001],            # Step size shrinkage to prevent overfitting\n",
    "            'xgboost__max_depth': [3, 5, 7],                         # Maximum depth of each tree\n",
    "            #'xgboost__n_estimators': [100, 200, 300],                # Number of boosting rounds\n",
    "            #'xgboost__min_child_weight': [1, 3, 5],                  # Minimum sum of instance weight needed in a child\n",
    "            #'xgboost__subsample': [0.6, 0.8,0.9],                   # Subsample ratio of the training instances\n",
    "            #'xgboost__colsample_bytree': [0.6, 0.8, 0.9],            # Subsample ratio of columns when constructing each tree\n",
    "            #'xgboost__gamma': [0.1, 0.5, 1,3],                     # Minimum loss reduction required for a split\n",
    "            #'xgboost__reg_alpha': [0.01, 0.1,0.5,1],                    # L1 regularization term on weights\n",
    "            #'xgboost__reg_lambda': [1, 1.5, 2],                      # L2 regularization term on weights\n",
    "            'xgboost__scale_pos_weight': [1, 5, 10]\n",
    "            }),\n",
    "    #'Logistic_Regression': (LogisticRegression(solver='liblinear'), {'logistic_regression__C': [0.1, 1, 10]}),\n",
    "    #'SVM': (SVC(probability=True), {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}),\n",
    "    #'NN': (MLPClassifier(max_iter=1000), {'nn__hidden_layer_sizes': [(10,), (50,), (100,)], 'nn__activation': ['relu', 'tanh']}),\n",
    "}\n",
    "\n",
    "# Initialize list to store predictions from each model\n",
    "ensemble_predictions = []\n",
    "ensemble_probabilities = []\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # Create a pipeline for preprocessing and model training\n",
    "    pipeline = Pipeline([\n",
    "        (model_name.lower(), model)\n",
    "    ])\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV with AUC score\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='precision',n_jobs=-1)\n",
    "    grid_search.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions and predict probabilities on the test set\n",
    "    y_pred = best_model.predict(X_test_final)\n",
    "    y_prob = best_model.predict_proba(X_test_final)[:, 1]  # Probability for the positive class\n",
    "    ensemble_predictions.append(y_pred)\n",
    "    ensemble_probabilities.append(y_prob)\n",
    "\n",
    "    # Evaluate the model using accuracy, F1-score, and AUC\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(f\"{model_name} F1 Score: {f1}\")\n",
    "    print(f\"{model_name} AUC: {auc}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Implement ensemble method (majority voting for predictions, average for probabilities)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_pred_final = mode(ensemble_predictions, axis=0)[0].flatten()\n",
    "\n",
    "# Average the probabilities for the ensemble AUC\n",
    "ensemble_probabilities = np.mean(ensemble_probabilities, axis=0)\n",
    "\n",
    "# Evaluate the ensemble model using accuracy, F1-score, and AUC\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred_final)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_pred_final)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_probabilities)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble F1 Score:\", ensemble_f1)\n",
    "print(\"Ensemble AUC:\", ensemble_auc)\n",
    "print(classification_report(y_test, ensemble_pred_final))\n",
    "\n",
    "with open('xgboost_tfidf_LLM_v5.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Body_Part</th>\n",
       "      <th>Location</th>\n",
       "      <th>Product_1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.688581</td>\n",
       "      <td>-0.935222</td>\n",
       "      <td>0.817527</td>\n",
       "      <td>-0.761139</td>\n",
       "      <td>-0.652828</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>0.670022</td>\n",
       "      <td>0.365266</td>\n",
       "      <td>-0.524021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044480</td>\n",
       "      <td>-1.460328</td>\n",
       "      <td>-0.969432</td>\n",
       "      <td>-0.424290</td>\n",
       "      <td>0.958737</td>\n",
       "      <td>0.771442</td>\n",
       "      <td>0.605041</td>\n",
       "      <td>1.809550</td>\n",
       "      <td>-0.886592</td>\n",
       "      <td>0.810894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127114</td>\n",
       "      <td>1.068401</td>\n",
       "      <td>1.211278</td>\n",
       "      <td>-0.761139</td>\n",
       "      <td>-1.261170</td>\n",
       "      <td>-0.067307</td>\n",
       "      <td>0.401968</td>\n",
       "      <td>-1.171033</td>\n",
       "      <td>-0.941745</td>\n",
       "      <td>1.006470</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.958180</td>\n",
       "      <td>0.542523</td>\n",
       "      <td>0.639321</td>\n",
       "      <td>-1.549186</td>\n",
       "      <td>3.117182</td>\n",
       "      <td>0.274781</td>\n",
       "      <td>1.060470</td>\n",
       "      <td>0.891147</td>\n",
       "      <td>0.856496</td>\n",
       "      <td>0.460138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.779871</td>\n",
       "      <td>-0.935222</td>\n",
       "      <td>0.467527</td>\n",
       "      <td>-0.437733</td>\n",
       "      <td>-0.264824</td>\n",
       "      <td>-2.116730</td>\n",
       "      <td>1.270491</td>\n",
       "      <td>0.623171</td>\n",
       "      <td>0.208979</td>\n",
       "      <td>-0.739689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>-0.933003</td>\n",
       "      <td>1.393133</td>\n",
       "      <td>-0.871157</td>\n",
       "      <td>0.060490</td>\n",
       "      <td>0.984578</td>\n",
       "      <td>0.372143</td>\n",
       "      <td>0.502606</td>\n",
       "      <td>-0.498860</td>\n",
       "      <td>-1.560613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.012504</td>\n",
       "      <td>1.068401</td>\n",
       "      <td>-1.413726</td>\n",
       "      <td>-0.437733</td>\n",
       "      <td>-0.264824</td>\n",
       "      <td>-0.649481</td>\n",
       "      <td>-0.746886</td>\n",
       "      <td>0.398183</td>\n",
       "      <td>1.166810</td>\n",
       "      <td>-1.296542</td>\n",
       "      <td>...</td>\n",
       "      <td>1.479026</td>\n",
       "      <td>0.052324</td>\n",
       "      <td>0.504815</td>\n",
       "      <td>-0.377094</td>\n",
       "      <td>-0.771888</td>\n",
       "      <td>-0.180674</td>\n",
       "      <td>-0.538727</td>\n",
       "      <td>0.210899</td>\n",
       "      <td>-1.125427</td>\n",
       "      <td>-0.448966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.796555</td>\n",
       "      <td>-0.935222</td>\n",
       "      <td>-1.194976</td>\n",
       "      <td>2.149511</td>\n",
       "      <td>-0.706985</td>\n",
       "      <td>-0.764863</td>\n",
       "      <td>0.125405</td>\n",
       "      <td>-0.634496</td>\n",
       "      <td>-1.264164</td>\n",
       "      <td>0.714660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080112</td>\n",
       "      <td>0.356393</td>\n",
       "      <td>-1.069071</td>\n",
       "      <td>0.654946</td>\n",
       "      <td>-0.433049</td>\n",
       "      <td>0.367173</td>\n",
       "      <td>0.967608</td>\n",
       "      <td>0.599836</td>\n",
       "      <td>0.599172</td>\n",
       "      <td>1.196029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age       Sex  Body_Part  Location  Product_1         0         1  \\\n",
       "0 -0.688581 -0.935222   0.817527 -0.761139  -0.652828  0.004202 -0.202524   \n",
       "1 -0.127114  1.068401   1.211278 -0.761139  -1.261170 -0.067307  0.401968   \n",
       "2  0.779871 -0.935222   0.467527 -0.437733  -0.264824 -2.116730  1.270491   \n",
       "3 -1.012504  1.068401  -1.413726 -0.437733  -0.264824 -0.649481 -0.746886   \n",
       "4 -0.796555 -0.935222  -1.194976  2.149511  -0.706985 -0.764863  0.125405   \n",
       "\n",
       "          2         3         4  ...       374       375       376       377  \\\n",
       "0  0.670022  0.365266 -0.524021  ...  0.044480 -1.460328 -0.969432 -0.424290   \n",
       "1 -1.171033 -0.941745  1.006470  ... -1.958180  0.542523  0.639321 -1.549186   \n",
       "2  0.623171  0.208979 -0.739689  ...  0.010165 -0.933003  1.393133 -0.871157   \n",
       "3  0.398183  1.166810 -1.296542  ...  1.479026  0.052324  0.504815 -0.377094   \n",
       "4 -0.634496 -1.264164  0.714660  ...  0.080112  0.356393 -1.069071  0.654946   \n",
       "\n",
       "        378       379       380       381       382       383  \n",
       "0  0.958737  0.771442  0.605041  1.809550 -0.886592  0.810894  \n",
       "1  3.117182  0.274781  1.060470  0.891147  0.856496  0.460138  \n",
       "2  0.060490  0.984578  0.372143  0.502606 -0.498860 -1.560613  \n",
       "3 -0.771888 -0.180674 -0.538727  0.210899 -1.125427 -0.448966  \n",
       "4 -0.433049  0.367173  0.967608  0.599836  0.599172  1.196029  \n",
       "\n",
       "[5 rows x 389 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.50122628, -1.45747622, -1.41372615, -1.36997608, -1.32622601,\n",
       "       -1.28247595, -1.23872588, -1.19497581, -1.15122575,  0.46752674,\n",
       "        0.5112768 ,  0.55502687,  0.642527  ,  0.68627707,  0.73002714,\n",
       "        0.77377721,  0.81752727,  1.03627761,  1.08002768,  1.21127788,\n",
       "        1.25502794,  1.29877801])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_body_parts = X_test_final['Body_Part'].unique()\n",
    "scaled_body_parts.sort()\n",
    "scaled_body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8180479 0.9804617\n",
      "0.8434001 0.9866623\n",
      "0.07610136 0.62177515\n",
      "0.3582114 0.96472424\n",
      "0.85777724 0.97853804\n",
      "0.5630455 0.95304084\n",
      "0.07579315 0.61503655\n",
      "0.6504902 0.9727951\n",
      "0.1762011 0.8535414\n",
      "0.5228751 0.9777255\n"
     ]
    }
   ],
   "source": [
    "for index, row in X_test_final[:10].iterrows():\n",
    "    prob = []\n",
    "    for body_part in scaled_body_parts:\n",
    "        current_sample = row.copy()\n",
    "        current_sample['Body_Part'] = body_part\n",
    "        # print(current_sample)\n",
    "        prob.append(best_model.predict_proba([current_sample])[0][0])\n",
    "    prob.sort()\n",
    "    print(prob[0], prob[-1]) # min max"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
